### this section is a continuation on the file 15
### we will parse file from the filebeat and filter it throught the logstash pipeline 

root@elk:/etc/logstash/conf.d# pwd
/etc/logstash/conf.d
root@elk:/etc/logstash/conf.d# ls
filebeat.conf
root@elk:/etc/logstash/conf.d#

### we are going to create iis filter, iis is part of window machine but we are going to show an example on the linux machine 
### on filebeat server create a directory iis
root@elk:/var/log# ls
README            apport.log  bootstrap.log  cloud-init-output.log  dist-upgrade  elasticsearch  filebeat        hello.log  journal  my_data.log  test.log   unattended-upgrades
alternatives.log  apt         btmp           cloud-init.log         dpkg.log      faillog        fontconfig.log  installer  lastlog  private      test1.log  wtmp
root@elk:/var/log# mkdir iis
root@elk:/var/log#

root@elk:/var/log# vim iis.log
root@elk:/var/log#

### add the sample iis log in iis.log file through iis logs grok and disect.docx document:
https://github.com/mussawirimam/Elastic_SearchChintaman/blob/main/iis%20logs%20grok%20and%20disect.docx

---------------------------------------------------------

### we already have apache pattern in the logstash pipeline, now we are targeting the iis pattern logs to parse the data through filebeat to logstash
# using grok debugger
TOOL: https://grokdebugger.com

root@elk:/var/log# rm -rf hello.log
root@elk:/var/log#


### in IIS we have a sample data and its grok pattern:
GROK on grokdebugger.

### IIS SAMPLE DATA
2024-03-21 14:30:00 W3SVC1 192.168.1.1 GET /index.html - 80 - 192.168.1.100 Mozilla/5.0 200 0 0 15

### OUTPUT AFTER PARSING
[
  {
    "log_timestamp": "2024-03-21 14:30:00",
    "sitename": "W3SVC1",
    "server_ip": "192.168.1.1",
    "method": "GET",
    "uri": "/index.html",
    "query": "-",
    "port": 80,
    "username": "-",
    "client_ip": "192.168.1.100",
    "user_agent": "Mozilla/5.0",
    "response_code": 200,
    "sub_status": 0,
    "win32_status": 0,
    "time_taken": 15
  }
]
----------------------------------------------
So now we have two types logstash pipelines on logstash server and two types of files on filebeat server.
We will use the help of the tags in our logstash pipline to further help us differentiate the data identification and parse the data through the elasticsearch 
we will use tags and if statement


when there are two files which you are trying to parse through the logstash conf.d files aka pipeline, you would need to add the tags and if and else condition in the logstash file
and at the filebeat config file, you will have to add the unique id among all input for apache and iis

### we need two inputs at the filebeat level configuration. 


### the importance of the tags in filebeat.yml
so if you have a tag apache in the filebeat.yml file you can then speicify in the logstash pipeline conf.d to target it
also, 
Separation → If Filebeat is collecting from multiple sources (e.g., nginx, system, mysql), you can assign different tags so you know where the logs originated.
Dashboards → Kibana visualizations/dashboards often rely on tags to differentiate log types.
