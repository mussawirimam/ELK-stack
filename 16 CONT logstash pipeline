### this section is a continuation on the file 15
### we will parse file from the filebeat and filter it throught the logstash pipeline 

root@elk:/etc/logstash/conf.d# pwd
/etc/logstash/conf.d
root@elk:/etc/logstash/conf.d# ls
filebeat.conf
root@elk:/etc/logstash/conf.d#

### we are going to create iis filter, iis is part of window machine but we are going to show an example on the linux machine 
### on filebeat server create a directory iis
root@elk:/var/log# ls
README            apport.log  bootstrap.log  cloud-init-output.log  dist-upgrade  elasticsearch  filebeat        hello.log  journal  my_data.log  test.log   unattended-upgrades
alternatives.log  apt         btmp           cloud-init.log         dpkg.log      faillog        fontconfig.log  installer  lastlog  private      test1.log  wtmp
root@elk:/var/log# mkdir iis
root@elk:/var/log#

root@elk:/var/log# vim iis.log
root@elk:/var/log#

### add the sample iis log in iis.log file through iis logs grok and disect.docx document:
https://github.com/mussawirimam/Elastic_SearchChintaman/blob/main/iis%20logs%20grok%20and%20disect.docx

---------------------------------------------------------

### we already have apache pattern in the logstash pipeline, now we are targeting the iis pattern logs to parse the data through filebeat to logstash
# using grok debugger
TOOL: https://grokdebugger.com

root@elk:/var/log# rm -rf hello.log
root@elk:/var/log#


### in IIS we have a sample data and its grok pattern:
GROK on grokdebugger.

### IIS SAMPLE DATA
2024-03-21 14:30:00 W3SVC1 192.168.1.1 GET /index.html - 80 - 192.168.1.100 Mozilla/5.0 200 0 0 15

### OUTPUT AFTER PARSING
[
  {
    "log_timestamp": "2024-03-21 14:30:00",
    "sitename": "W3SVC1",
    "server_ip": "192.168.1.1",
    "method": "GET",
    "uri": "/index.html",
    "query": "-",
    "port": 80,
    "username": "-",
    "client_ip": "192.168.1.100",
    "user_agent": "Mozilla/5.0",
    "response_code": 200,
    "sub_status": 0,
    "win32_status": 0,
    "time_taken": 15
  }
]
----------------------------------------------
So now we have two types logstash pipelines on logstash server and two types of files on filebeat server.
We will use the help of the tags in our logstash pipline to further help us differentiate the data identification and parse the data through the elasticsearch 
we will use tags and if statement

