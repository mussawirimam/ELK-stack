# we will install filebeat and on one server we will install logstash and make the connectivity and send the file from filebeat to the logstash. 

filebeat logs files and send the data to the either to the elasticsearch or to the logstash

IMPORTANT: why you send the data to the logstash?
If there is any mutation that needs to be done in the data, we have to parse the data in a structured way. So, we send the data to Logstash, and Logstash hands it over to Elasticsearch. The data can then be visualized on the Kibana Discover page.

we will continue after the installation of the file beat, you check the installation steps in document 13. 

we will conigure the logstash below

---------------------------------------------------------------------------------------------
LOGSTASH INSTALLATION
---------------------------------------------------------------------------------------------
Installation steps are in this file in your githubrepository
Elastic_SearchChintaman/Filebeat and Logstash installtion.docx
https://github.com/mussawirimam/Elastic_SearchChintaman/blob/main/Filebeat%20and%20Logstash%20installtion.docx
remember you installed the filebeat on the node6
install the logstash on the node 5, you can also create another server for the logstash but we are going to use the node 5

Here are the steps to install Logstash on the latest Ubuntu version:
Step 1: Update System Packages
sh
sudo apt update && sudo apt upgrade -y
Step 2: Install Java (if not installed)
Logstash requires Java. Install OpenJDK:
sh

sudo apt install -y openjdk-17-jdk  <--------------------- ### if you have installed the elasticsearch before, you can skip this step ### we need java since the logstash is build in java, so we need a java runtime. 
Verify Java installation:
sh

java -version
Step 3: Add Elasticsearch Repository
sh

wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo tee /usr/share/keyrings/elasticsearch-keyring.asc
echo "deb [signed-by=/usr/share/keyrings/elasticsearch-keyring.asc] https://artifacts.elastic.co/packages/8.x/apt stable main" | sudo tee /etc/apt/sources.list.d/elastic-8.x.list
Step 4: Install Logstash
sh

sudo apt update
sudo apt install -y logstash
Step 5: Configure Logstash
Edit the main configuration file:
sh

sudo nano /etc/logstash/logstash.yml
Modify settings as needed (e.g., pipeline configurations).
Step 6: Configure Logstash Pipeline
Create a sample pipeline:
sh

sudo nano /etc/logstash/conf.d/sample.conf
Add an example pipeline configuration:
plaintext

input {
  beats {
    port => 5044
  }
}

filter {
  mutate {
    add_field => { "example_field" => "Hello Logstash" }
  }
}

output {
  elasticsearch {
    hosts => ["http://localhost:9200"]
    index => "logstash-test"
  }
  stdout { codec => rubydebug }
}
Save and exit.
Step 7: Enable and Start Logstash
sh

sudo systemctl enable logstash
sudo systemctl start logstash
Check service status:
sh

sudo systemctl status logstash
Step 8: Verify Logstash
Test Logstash manually:
sh

echo '{"message": "Hello, Logstash!"}' | /usr/share/logstash/bin/logstash -f /etc/logstash/conf.d/sample.conf
Step 9: Allow Ports in Firewall (If Required)
sh

sudo ufw allow 5044/tcp
sudo ufw reload
Step 10: Monitor Logstash Logs
sh

sudo journalctl -u logstash -f
Let me know if you need further customization! 



################# CONFIGURATION OF LOGSTASH
root@elk:/etc/logstash# ls
conf.d  jvm.options  log4j2.properties  logstash-sample.conf  logstash.yml  pipelines.yml  startup.options
root@elk:/etc/logstash#

these 3 files are for the system configureation, we dont deal with them but time to time incident can arise where you will check these files.
jvm.options = we use it for java application settings
log4j2.properties = is used for logging purpose, generating the logs of the logstash specification
startup.option = steps of what will happen first, there are variable values 
  e.g when the logstash process will run, it will create a pid. This process will be captured in /var/run/logstash.pid etc

main 3 configuration files
logstash.yml = all the features of the logstash, we can enable and disable through this file
  e.g logfile or search settings etc
  # in our case we are not running logstash cluster but single node cluster for logstash
pipeline.yml = to manage pipeline related configuration
conf.d = is related to manage the pipelines, we use it with combination of pipeline.yml file 
  pip1.conf
  pip2.conf
  pip3.conf

currently we will work with the single pipeline which will to open the filebeat port recieve logs. 
pipline.yml file you can specify form where to read the files, you can add a custom folder or use the conf.d which comes default with the logstash installation. 

logstash uses logstash.yml file for the confuration
logstash uses pipline.yml file for all the pipelines

do homework on persisted que from the logstash.yml file what it does and why we need it. 

persistence vs queue:
persistence is handled by the logstash and the queue is handled by us. 

------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
logstash.yml file example:
root@elk:/etc/logstash# cat logstash.yml
# Settings file in YAML
#
# Settings can be specified either in hierarchical form, e.g.:
#
#   pipeline:
#     batch:
#       size: 125
#       delay: 5
#
# Or as flat keys:
#
#   pipeline.batch.size: 125
#   pipeline.batch.delay: 5
#
# ------------  Node identity ------------
#
# Use a descriptive name for the node:
#
# node.name: test
#
# If omitted the node name will default to the machine's host name
#
# ------------ Data path ------------------
#
# Which directory should be used by logstash and its plugins
# for any persistent needs. Defaults to LOGSTASH_HOME/data
#
path.data: /var/lib/logstash
#
# ------------ Pipeline Settings --------------
#
# The ID of the pipeline.
#
# pipeline.id: main
#
# Set the number of workers that will, in parallel, execute the filters+outputs
# stage of the pipeline.
#
# This defaults to the number of the host's CPU cores.
#
# pipeline.workers: 2      #<--------------------------------------------------------------------------- ### we can use two workers parallely to get the load off and to make the pipeline work fast. We set the configuration for pipeline in pipeline.yml so its manageable (main file for pipeline). 
#
# How many events to retrieve from inputs before sending to filters+workers
#
# pipeline.batch.size: 125
#
# How long to wait in milliseconds while polling for the next event
# before dispatching an undersized batch to filters+outputs
#
# pipeline.batch.delay: 50
#
# Force Logstash to exit during shutdown even if there are still inflight
# events in memory. By default, logstash will refuse to quit until all
# received events have been pushed to the outputs.
#
# WARNING: Enabling this can lead to data loss during shutdown
#
# pipeline.unsafe_shutdown: false
#
# Set the pipeline event ordering. Options are "auto" (the default), "true" or "false".
# "auto" automatically enables ordering if the 'pipeline.workers' setting
# is also set to '1', and disables otherwise.
# "true" enforces ordering on the pipeline and prevent logstash from starting
# if there are multiple workers.
# "false" disables any extra processing necessary for preserving ordering.
#
# pipeline.ordered: auto
#
# Sets the pipeline's default value for `ecs_compatibility`, a setting that is
# available to plugins that implement an ECS Compatibility mode for use with
# the Elastic Common Schema.
# Possible values are:
# - disabled
# - v1
# - v8 (default)
# Pipelines defined before Logstash 8 operated without ECS in mind. To ensure a
# migrated pipeline continues to operate as it did before your upgrade, opt-OUT
# of ECS for the individual pipeline in its `pipelines.yml` definition. Setting
# it here will set the default for _all_ pipelines, including new ones.
#
# pipeline.ecs_compatibility: v8
#
# ------------ Pipeline Configuration Settings --------------
#
# Where to fetch the pipeline configuration for the main pipeline
#
# path.config:
#
# Pipeline configuration string for the main pipeline
#
# config.string:
#
# At startup, test if the configuration is valid and exit (dry run)
#
# config.test_and_exit: false
#
# Periodically check if the configuration has changed and reload the pipeline
# This can also be triggered manually through the SIGHUP signal
#
# config.reload.automatic: false
#
# How often to check if the pipeline configuration has changed (in seconds)
# Note that the unit value (s) is required. Values without a qualifier (e.g. 60)
# are treated as nanoseconds.
# Setting the interval this way is not recommended and might change in later versions.
#
# config.reload.interval: 3s
#
# Show fully compiled configuration as debug log message
# NOTE: --log.level must be 'debug'
#
# config.debug: false
#
# When enabled, process escaped characters such as \n and \" in strings in the
# pipeline configuration files.
#
# config.support_escapes: false
#
# ------------ API Settings -------------        ###<----------------------------------- whenever the logstash starts, it creates the api same as the elasticsearch to deal with the logstash. Which can be exposed on the api.http.host and we can change the port as well.
# Define settings related to the HTTP API here.
#
# The HTTP API is enabled by default. It can be disabled, but features that rely
# on it will not work as intended.
#
# api.enabled: true
#
# By default, the HTTP API is not secured and is therefore bound to only the
# host's loopback interface, ensuring that it is not accessible to the rest of
# the network.
# When secured with SSL and Basic Auth, the API is bound to _all_ interfaces
# unless configured otherwise.
#
# api.http.host: 127.0.0.1
#
# The HTTP API web server will listen on an available port from the given range.
# Values can be specified as a single port (e.g., `9600`), or an inclusive range
# of ports (e.g., `9600-9700`).
#
# api.http.port: 9600-9700
#
# The HTTP API includes a customizable "environment" value in its response,
# which can be configured here.
#
# api.environment: "production"
#
# The HTTP API can be secured with SSL (TLS). To do so, you will need to provide
# the path to a password-protected keystore in p12 or jks format, along with credentials.
#
# api.ssl.enabled: false
# api.ssl.keystore.path: /path/to/keystore.jks
# api.ssl.keystore.password: "y0uRp4$$w0rD"
#
# The availability of SSL/TLS protocols depends on the JVM version. Certain protocols are
# disabled by default and need to be enabled manually by changing `jdk.tls.disabledAlgorithms`
# in the $JDK_HOME/conf/security/java.security configuration file.
#
# api.ssl.supported_protocols: [TLSv1.2,TLSv1.3]
#
# The HTTP API can be configured to require authentication. Acceptable values are
#  - `none`:  no auth is required (default)
#  - `basic`: clients must authenticate with HTTP Basic auth, as configured
#             with `api.auth.basic.*` options below
# api.auth.type: none
#
# When configured with `api.auth.type` `basic`, you must provide the credentials
# that requests will be validated against. Usage of Environment or Keystore
# variable replacements is encouraged (such as the value `"${HTTP_PASS}"`, which
# resolves to the value stored in the keystore's `HTTP_PASS` variable if present
# or the same variable from the environment)
#
# api.auth.basic.username: "logstash-user"
# api.auth.basic.password: "s3cUreP4$$w0rD"
#
# When setting `api.auth.basic.password`, the password should meet
# the default password policy requirements.
# The default password policy requires non-empty minimum 8 char string that
# includes a digit, upper case letter and lower case letter.
# Policy mode sets Logstash to WARN or ERROR when HTTP authentication password doesn't
# meet the password policy requirements.
# The default is WARN. Setting to ERROR enforces stronger passwords (recommended).
#
# api.auth.basic.password_policy.mode: WARN
#
# ------------ Module Settings ---------------
# Define modules here.  Modules definitions must be defined as an array.
# The simple way to see this is to prepend each `name` with a `-`, and keep
# all associated variables under the `name` they are associated with, and
# above the next, like this:
#
# modules:
#   - name: MODULE_NAME
#     var.PLUGINTYPE1.PLUGINNAME1.KEY1: VALUE
#     var.PLUGINTYPE1.PLUGINNAME1.KEY2: VALUE
#     var.PLUGINTYPE2.PLUGINNAME1.KEY1: VALUE
#     var.PLUGINTYPE3.PLUGINNAME3.KEY1: VALUE
#
# Module variable names must be in the format of
#
# var.PLUGIN_TYPE.PLUGIN_NAME.KEY
#
# modules:
#
# ------------ Cloud Settings ---------------
# Define Elastic Cloud settings here.
# Format of cloud.id is a base64 value e.g. dXMtZWFzdC0xLmF3cy5mb3VuZC5pbyRub3RhcmVhbCRpZGVudGlmaWVy
# and it may have an label prefix e.g. staging:dXMtZ...
# This will overwrite 'var.elasticsearch.hosts' and 'var.kibana.host'
# cloud.id: <identifier>
#
# Format of cloud.auth is: <user>:<pass>
# This is optional
# If supplied this will overwrite 'var.elasticsearch.username' and 'var.elasticsearch.password'
# If supplied this will overwrite 'var.kibana.username' and 'var.kibana.password'
# cloud.auth: elastic:<password>
#
# ------------ Queuing Settings --------------
#
# Internal queuing model, "memory" for legacy in-memory based queuing and
# "persisted" for disk-based acked queueing. Defaults is memory
#
# queue.type: memory
#
# If `queue.type: persisted`, the directory path where the pipeline data files will be stored.
# Each pipeline will group its PQ files in a subdirectory matching its `pipeline.id`.
# Default is path.data/queue.
#
# path.queue:  ###<-------------------------------------------------------------------------------------------- what queing setting is: in your logstash you are going to use input, filter, and output. So to work with this, we specify if the name is not in tag then go to xyz queue. We can specify our queues, queue gets utilized for unhandled data. If there is unhandled exception, we can not send it to the elasticsearch but to dead letter queue. 
#
# If using queue.type: persisted, the page data files size. The queue data consists of
# append-only data files separated into pages. Default is 64mb
#
# queue.page_capacity: 64mb
#
# If using queue.type: persisted, the maximum number of unread events in the queue.
# Default is 0 (unlimited)
#
# queue.max_events: 0
#
# If using queue.type: persisted, the total capacity of the queue in number of bytes.
# If you would like more unacked events to be buffered in Logstash, you can increase the
# capacity using this setting. Please make sure your disk drive has capacity greater than
# the size specified here. If both max_bytes and max_events are specified, Logstash will pick
# whichever criteria is reached first
# Default is 1024mb or 1gb
#
# queue.max_bytes: 1024mb
#
# If using queue.type: persisted, the maximum number of acked events before forcing a checkpoint
# Default is 1024, 0 for unlimited
#
# queue.checkpoint.acks: 1024
#
# If using queue.type: persisted, the maximum number of written events before forcing a checkpoint
# Default is 1024, 0 for unlimited
#
# queue.checkpoint.writes: 1024
#
# If using queue.type: persisted, the interval in milliseconds when a checkpoint is forced on the head page
# Default is 1000, 0 for no periodic checkpoint.
#
# queue.checkpoint.interval: 1000
#
# ------------ Dead-Letter Queue Settings --------------
# Flag to turn on dead-letter queue.
#
# dead_letter_queue.enable: false

# If using dead_letter_queue.enable: true, the maximum size of each dead letter queue. Entries
# will be dropped if they would increase the size of the dead letter queue beyond this setting.
# Default is 1024mb
# dead_letter_queue.max_bytes: 1024mb

# If using dead_letter_queue.enable: true, the interval in milliseconds where if no further events eligible for the DLQ
# have been created, a dead letter queue file will be written. A low value here will mean that more, smaller, queue files
# may be written, while a larger value will introduce more latency between items being "written" to the dead letter queue, and
# being available to be read by the dead_letter_queue input when items are written infrequently.
# Default is 5000.
#
# dead_letter_queue.flush_interval: 5000

# If using dead_letter_queue.enable: true, controls which entries should be dropped to avoid exceeding the size limit.
# Set the value to `drop_newer` (default) to stop accepting new events that would push the DLQ size over the limit.
# Set the value to `drop_older` to remove queue pages containing the oldest events to make space for new ones.
#
# dead_letter_queue.storage_policy: drop_newer

# If using dead_letter_queue.enable: true, the interval that events have to be considered valid. After the interval has
# expired the events could be automatically deleted from the DLQ.
# The interval could be expressed in days, hours, minutes or seconds, using as postfix notation like 5d,
# to represent a five days interval.
# The available units are respectively d, h, m, s for day, hours, minutes and seconds.
# If not specified then the DLQ doesn't use any age policy for cleaning events.
#
# dead_letter_queue.retain.age: 1d

# If using dead_letter_queue.enable: true, the directory path where the data files will be stored.
# Default is path.data/dead_letter_queue
#
# path.dead_letter_queue:
#
# ------------ Debugging Settings --------------
#
# Options for log.level:
#   * fatal
#   * error
#   * warn
#   * info (default)
#   * debug
#   * trace
# log.level: info
#
# Options for log.format:
#   * plain (default)
#   * json
#
# log.format: plain
# log.format.json.fix_duplicate_message_fields: false
#
path.logs: /var/log/logstash
#
# ------------ Other Settings --------------
#
# Allow or block running Logstash as superuser (default: true)
# allow_superuser: false
#
# Where to find custom plugins
# path.plugins: []
#
# Flag to output log lines of each pipeline in its separate log file. Each log filename contains the pipeline.name
# Default is false
# pipeline.separate_logs: false
#
# Determine where to allocate memory buffers, for plugins that leverage them.
# Default to direct, optionally can be switched to heap to select Java heap space.
# pipeline.buffer.type: heap
#
# ------------ X-Pack Settings (not applicable for OSS build)--------------
#
# X-Pack Monitoring
# https://www.elastic.co/guide/en/logstash/current/monitoring-logstash.html
#xpack.monitoring.enabled: false
#xpack.monitoring.elasticsearch.username: logstash_system
#xpack.monitoring.elasticsearch.password: password
#xpack.monitoring.elasticsearch.proxy: ["http://proxy:port"]
#xpack.monitoring.elasticsearch.hosts: ["https://es1:9200", "https://es2:9200"]
# an alternative to hosts + username/password settings is to use cloud_id/cloud_auth
#xpack.monitoring.elasticsearch.cloud_id: monitoring_cluster_id:xxxxxxxxxx
#xpack.monitoring.elasticsearch.cloud_auth: logstash_system:password
# another authentication alternative is to use an Elasticsearch API key
#xpack.monitoring.elasticsearch.api_key: "id:api_key"
#xpack.monitoring.elasticsearch.ssl.certificate_authority: "/path/to/ca.crt"
#xpack.monitoring.elasticsearch.ssl.ca_trusted_fingerprint: xxxxxxxxxx
#xpack.monitoring.elasticsearch.ssl.truststore.path: path/to/file
#xpack.monitoring.elasticsearch.ssl.truststore.password: password
# use either keystore.path/keystore.password or certificate/key configurations
#xpack.monitoring.elasticsearch.ssl.keystore.path: /path/to/file
#xpack.monitoring.elasticsearch.ssl.keystore.password: password
#xpack.monitoring.elasticsearch.ssl.certificate: /path/to/file
#xpack.monitoring.elasticsearch.ssl.key: /path/to/key
#xpack.monitoring.elasticsearch.ssl.verification_mode: full
#xpack.monitoring.elasticsearch.ssl.cipher_suites: []
#xpack.monitoring.elasticsearch.sniffing: false
#xpack.monitoring.collection.interval: 10s
#xpack.monitoring.collection.pipeline.details.enabled: true
#
# X-Pack Management
# https://www.elastic.co/guide/en/logstash/current/logstash-centralized-pipeline-management.html
#xpack.management.enabled: false
#xpack.management.pipeline.id: ["main", "apache_logs"]
#xpack.management.elasticsearch.username: logstash_admin_user
#xpack.management.elasticsearch.password: password
#xpack.management.elasticsearch.proxy: ["http://proxy:port"]
#xpack.management.elasticsearch.hosts: ["https://es1:9200", "https://es2:9200"]
# an alternative to hosts + username/password settings is to use cloud_id/cloud_auth
#xpack.management.elasticsearch.cloud_id: management_cluster_id:xxxxxxxxxx
#xpack.management.elasticsearch.cloud_auth: logstash_admin_user:password
# another authentication alternative is to use an Elasticsearch API key
#xpack.management.elasticsearch.api_key: "id:api_key"
#xpack.management.elasticsearch.ssl.ca_trusted_fingerprint: xxxxxxxxxx
#xpack.management.elasticsearch.ssl.certificate_authority: "/path/to/ca.crt"
#xpack.management.elasticsearch.ssl.truststore.path: /path/to/file
#xpack.management.elasticsearch.ssl.truststore.password: password
# use either keystore.path/keystore.password or certificate/key configurations
#xpack.management.elasticsearch.ssl.keystore.path: /path/to/file
#xpack.management.elasticsearch.ssl.keystore.password: password
#xpack.management.elasticsearch.ssl.certificate: /path/to/file
#xpack.management.elasticsearch.ssl.key: /path/to/certificate_key_file
#xpack.management.elasticsearch.ssl.cipher_suites: []
#xpack.management.elasticsearch.ssl.verification_mode: full
#xpack.management.elasticsearch.sniffing: false
#xpack.management.logstash.poll_interval: 5s

# X-Pack GeoIP Database Management
# https://www.elastic.co/guide/en/logstash/current/plugins-filters-geoip.html#plugins-filters-geoip-manage_update
#xpack.geoip.downloader.enabled: true
#xpack.geoip.downloader.endpoint: "https://geoip.elastic.co/v1/database"
root@elk:/etc/logstash#

------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
### On pipeline

root@elk:/etc/logstash# ls
conf.d  jvm.options  log4j2.properties  logstash-sample.conf  logstash.yml  pipelines.yml  startup.options



### how logstash will make sure which pipeline id we should run?
### it will look into more logstash.yml file and in that the pipeline.id: main is mentioned in both the files. 
### but when we work with the mutliple pipelines, suppose in your pipeline the first id is main and second id can be xyz you can mention multiple id's based on multiple pipeline in different directories as well. 
### you can add other files in the pipeline.yml file as well
### from pipelines.yml we can read our config files path and it's location
### we need to configure our filebeat config file in the /etc/logstash/conf.d <---------------------

root@elk:/etc/logstash# cat pipelines.yml
# This file is where you define your pipelines. You can define multiple.
# For more information on multiple pipelines, see the documentation:
#   https://www.elastic.co/guide/en/logstash/current/multiple-pipelines.html

- pipeline.id: main
  path.config: "/etc/logstash/conf.d/*.conf"
root@elk:/etc/logstash#

root@elk:/etc/logstash# cat logstash.yml | grep main
# pipeline.id: main
# Where to fetch the pipeline configuration for the main pipeline
# Pipeline configuration string for the main pipeline
#xpack.management.pipeline.id: ["main", "apache_logs"]
root@elk:/etc/logstash#

### we need to configure our filebeat config file in the /etc/logstash/conf.d <---------------------
### the file extension of the file should be .conf

### we have three parts to create the structure of the conf.d configuration file: Input, filter, and Output.
https://www.elastic.co/docs/reference/logstash/plugins/plugins-inputs-beats
https://www.elastic.co/docs/reference/logstash/plugins/plugins-outputs-elasticsearch <------------- you can see the settings and password input type 

root@elk:/etc/logstash# ls
conf.d  jvm.options  log4j2.properties  logstash-sample.conf  logstash.yml  pipelines.yml  startup.options
root@elk:/etc/logstash# cd conf.d/
root@elk:/etc/logstash/conf.d# vim filebeat.conf

input {
  beats {
    port => 5044
  }
}

filter {

}

output {
  elasticsearch {
    hosts => ["http://localhost:9200"]
    index => "%{[@metadata][beat]}-%{[@metadata][version]}"
  }
}


### we will configure the ssl within the filebeat.conf as well
https://www.elastic.co/docs/reference/fleet/secure-logstash-connections
### now as we are using self-signed certificate ssl, so we have to provide ssl certificate here as well to elasticsearch
### we need to provide ssl true, you can see it in the official document and provide the CA as well
## we are using same CA through which we generated all the Certificates
input {
  beats {
    port => 5044
  }
}

filter {

}

output {
  elasticsearch {
    hosts => ["http://10.211.55.32:9200"] <---------------------------------------- ip address of the co-ordinator node where the filebeat is installed.
    index => "%{[@metadata][beat]}-%{[@metadata][version]}"
    ssl => true
    cacert => "/path/to/http_ca.crt"  <-------------------------- we need provide the ca through which we generated all the certificates, we previously generated in the elasticsearch configuration. (CA utilized to generate all the certificates.)
  }
}

### we also need to provide ID and Password
### side note: for security reason we can use environment variable as well. 
### side note: do not specify index name anywhere in capital letters
### but here we dont want api-key but ID password
### these are the mandatory parameters below

input {
  beats {
    port => 5044

  }
}

filter {

}

output {
  elasticsearch {
    hosts => ["http://10.211.55.32:9200"]
    index => "logs_for_test_%{+YYYY.MM.dd}"  <------------------------ pattern of the index, index name needs to be always in small letters
    user => elastic <------------------------------------------------- you can create another user and add the user here
    password => blfOwgJUMUuLKjmXVYU2
    ssl => true
    cacert => "/path/to/http_ca.crt"
  }
}

#### IMPORTANT ON NEXT: 
### now we will configure the filebeat and we will read data and send it to port 5044 and we will see the data coming to the elasticsearch
###
root@elk:/etc/logstash/conf.d# ifconfig |grep inet
        inet 10.211.55.31 <-------------- logstash/elasticsearch node-5/data node

# to verify if our node is able to communicate/connected with the elasticsearch port
root@elk:/etc/logstash/conf.d# telnet 10.211.55.32:9200
Server lookup failure:  10.211.55.32:9200:telnet, Name or service not known
root@elk:/etc/logstash/conf.d# telnet 10.211.55.32 9200
Trying 10.211.55.32...
Connected to 10.211.55.32.
Escape character is '^]'.

### with providing ca crt to curl to verify certificate 
### to verify the certificate, we can use this curl call
curl -u elastic:<password here> https://sys.fcc:9200 --cacert /etc/elasticsearch/certs/ca/ca.crt

-----------------------------------------------------------------------------------------------------
TROUBLE SHOOTING TIME:
-----------------------------------------------------------------------------------------------------
ERROR:
root@elk:/etc/logstash/conf.d# curl -u elastic:blfOwgJUMUuLKjmXVYU2 https://sys.fcc:9200 --cacert /etc/elasticsearch/certs/ca/ca.crt
curl: (7) Failed to connect to sys.fcc port 9200 after 3061 ms: Couldn't connect to server
root@elk:/etc/logstash/conf.d#

RESOLUTION:
### add the ip address of the node-6 in the /etc/hosts of node-5
root@elk:/usr/share/elasticsearch/bin# ifconfig
enp0s5: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
        inet 10.211.55.32

### on node-5
root@elk:/etc/logstash/conf.d# vi /etc/hosts
10.211.55.32    sys.fcc
:wq!

root@elk:/etc/logstash/conf.d# curl -u elastic:blfOwgJUMUuLKjmXVYU2 https://sys.fcc:9200 --cacert /etc/elasticsearch/certs/ca/ca.crt
{
  "name" : "node-6",
  "cluster_name" : "future",
  "cluster_uuid" : "N5G75_FYSm-fp-cvIrhXGQ",
  "version" : {
    "number" : "8.18.1",
    "build_flavor" : "default",
    "build_type" : "deb",
    "build_hash" : "df116ec6455476a07daafc3ded80e2bb1a3385ed",
    "build_date" : "2025-04-30T10:07:44.026929518Z",
    "build_snapshot" : false,
    "lucene_version" : "9.12.1",
    "minimum_wire_compatibility_version" : "7.17.0",
    "minimum_index_compatibility_version" : "7.0.0"
  },
  "tagline" : "You Know, for Search"
}
root@elk:/etc/logstash/conf.d#
### the reason it worked is because we generated the certificates with the sys.fcc
### just because we are working with the self-signed certificate, we are providing the ca

### to proceed with the for insecure connection
root@elk:/etc/logstash/conf.d# curl -u elastic:blfOwgJUMUuLKjmXVYU2 https://sys.fcc:9200 -k
{
  "name" : "node-6",
  "cluster_name" : "future",
  "cluster_uuid" : "N5G75_FYSm-fp-cvIrhXGQ",
  "version" : {
    "number" : "8.18.1",
    "build_flavor" : "default",
    "build_type" : "deb",
    "build_hash" : "df116ec6455476a07daafc3ded80e2bb1a3385ed",
    "build_date" : "2025-04-30T10:07:44.026929518Z",
    "build_snapshot" : false,
    "lucene_version" : "9.12.1",
    "minimum_wire_compatibility_version" : "7.17.0",
    "minimum_index_compatibility_version" : "7.0.0"
  },
  "tagline" : "You Know, for Search"
}
root@elk:/etc/logstash/conf.d#

### 10.211.55.32	sys.fcc we are resolving the domainname ane ip with the  ca
sys.fcc
or 
10.211.55.32
means the same thing which belongs to the same server which is our cordinator node
### since we are working in the private network, that is why we are using sys.fcc which we resolve with the host entry with ip to hostname
### how to resolve the domain name? through our ca certificate

-----------------------------------------------------------------------------------------------------
TROUBLE SHOOTING TIME ENDS
-----------------------------------------------------------------------------------------------------

-------------------------------
### STARTING LOGSTASH MANUALLY:
-------------------------------
### whenever we are testing the logstash we should manually check whether our configuration file going to work or not.
## we are going to go to logstash binary director to test

root@elk:/usr/share/logstash# cd /usr/share/logstash/bin
root@elk:/usr/share/logstash/bin# ls
benchmark.bat  dependencies-report  logstash           logstash-keystore.bat  logstash-plugin.bat  pqrepair      setup.bat
benchmark.sh   ingest-convert.bat   logstash.bat       logstash.lib.sh        pqcheck              pqrepair.bat  system-install
cpdump         ingest-convert.sh    logstash-keystore  logstash-plugin        pqcheck.bat          ruby
root@elk:/usr/share/logstash/bin#

# logstash-plugin file allows you to download more plugins that arent available on the logstash system
# when you run the logstash through bin, it will try to read the config of logstash.yml and pipeline.yml and it will try to send the data to the configure output
# it will not read any pipeline.yml, it will only read the config files only
root@elk:/usr/share/logstash/bin# ./logstash -f /etc/logstash/conf.d/filebeat.conf
Using bundled JDK: /usr/share/logstash/jdk
OpenJDK 64-Bit Server VM warning: Unable to get SVE vector length on this system. Disabling SVE. Specify -XX:UseSVE=0 to shun this warning.
OpenJDK 64-Bit Server VM warning: Unable to get SVE vector length on this system. Disabling SVE. Specify -XX:UseSVE=0 to shun this warning.
WARNING: Could not find logstash.yml which is typically located in $LS_HOME/config or /etc/logstash. You can specify the path using --path.settings. Continuing using the defaults
Could not find log4j2 configuration at path /usr/share/logstash/config/log4j2.properties. Using default config which logs errors to the console
[WARN ] 2025-07-28 03:24:30.308 [main] runner - Starting from version 9.0, running with superuser privileges is not permitted unless you explicitly set 'allow_superuser' to true, thereby acknowledging the possible security risks
[WARN ] 2025-07-28 03:24:30.309 [main] runner - NOTICE: Running Logstash as a superuser is strongly discouraged as it poses a security risk. Set 'allow_superuser' to false for better security.
[WARN ] 2025-07-28 03:24:30.311 [main] runner - 'pipeline.buffer.type' setting is not explicitly defined.Before moving to 9.x set it to 'heap' and tune heap size upward, or set it to 'direct' to maintain existing behavior.
[INFO ] 2025-07-28 03:24:30.312 [main] runner - Starting Logstash {"logstash.version"=>"8.18.4", "jruby.version"=>"jruby 9.4.9.0 (3.1.4) 2024-11-04 547c6b150e OpenJDK 64-Bit Server VM 21.0.7+6-LTS on 21.0.7+6-LTS +indy +jit [aarch64-linux]"}
[INFO ] 2025-07-28 03:24:30.312 [main] runner - JVM bootstrap flags: [-Xms1g, -Xmx1g, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djruby.compile.invokedynamic=true, -XX:+HeapDumpOnOutOfMemoryError, -Djava.security.egd=file:/dev/urandom, -Dlog4j2.isThreadContextMapInheritable=true, -Djruby.regexp.interruptible=true, -Djdk.io.File.enableADS=true, --add-exports=jdk.compiler/com.sun.tools.javac.api=ALL-UNNAMED, --add-exports=jdk.compiler/com.sun.tools.javac.file=ALL-UNNAMED, --add-exports=jdk.compiler/com.sun.tools.javac.parser=ALL-UNNAMED, --add-exports=jdk.compiler/com.sun.tools.javac.tree=ALL-UNNAMED, --add-exports=jdk.compiler/com.sun.tools.javac.util=ALL-UNNAMED, --add-opens=java.base/java.security=ALL-UNNAMED, --add-opens=java.base/java.io=ALL-UNNAMED, --add-opens=java.base/java.nio.channels=ALL-UNNAMED, --add-opens=java.base/sun.nio.ch=ALL-UNNAMED, --add-opens=java.management/sun.management=ALL-UNNAMED, -Dio.netty.allocator.maxOrder=11]
[INFO ] 2025-07-28 03:24:30.399 [main] StreamReadConstraintsUtil - Jackson default value override `logstash.jackson.stream-read-constraints.max-string-length` configured to `200000000` (logstash default)
[INFO ] 2025-07-28 03:24:30.399 [main] StreamReadConstraintsUtil - Jackson default value override `logstash.jackson.stream-read-constraints.max-number-length` configured to `10000` (logstash default)
[INFO ] 2025-07-28 03:24:30.399 [main] StreamReadConstraintsUtil - Jackson default value override `logstash.jackson.stream-read-constraints.max-nesting-depth` configured to `1000` (logstash default)
[INFO ] 2025-07-28 03:24:30.405 [main] settings - Creating directory {:setting=>"path.queue", :path=>"/usr/share/logstash/data/queue"}
[INFO ] 2025-07-28 03:24:30.406 [main] settings - Creating directory {:setting=>"path.dead_letter_queue", :path=>"/usr/share/logstash/data/dead_letter_queue"}
[WARN ] 2025-07-28 03:24:30.526 [LogStash::Runner] multilocal - Ignoring the 'pipelines.yml' file because modules or command line options are specified
[INFO ] 2025-07-28 03:24:30.535 [LogStash::Runner] agent - No persistent UUID file found. Generating new UUID {:uuid=>"ac56f82a-a13f-4f27-ba9f-65d9aa6d890c", :path=>"/usr/share/logstash/data/uuid"}
[INFO ] 2025-07-28 03:24:30.812 [Api Webserver] agent - Successfully started Logstash API endpoint {:port=>9600, :ssl_enabled=>false}
[INFO ] 2025-07-28 03:24:30.914 [Converge PipelineAction::Create<main>] Reflections - Reflections took 55 ms to scan 1 urls, producing 150 keys and 529 values
[WARN ] 2025-07-28 03:24:31.190 [Converge PipelineAction::Create<main>] elasticsearch - You are using a deprecated config setting "cacert" set in elasticsearch. Deprecated settings will continue to work, but are scheduled for removal from logstash in the future. Set 'ssl_certificate_authorities' instead. If you have any questions about this, please ask it on the https://discuss.elastic.co/c/logstash discussion forum {:name=>"cacert", :plugin=><LogStash::Outputs::ElasticSearch password=><password>, hosts=>[http://10.211.55.32:9200], cacert=>"/etc/elasticsearch/certs/ca/ca.crt", index=>"logs_for_test_%{+YYYY.MM.dd}", id=>"36015095d477a4f76f2ded08381460cd843ca7cf5f277090662bc7e644f76134", user=>"elastic", ssl=>true, enable_metric=>true, codec=><LogStash::Codecs::Plain id=>"plain_2586a0ce-200a-47db-999e-573711d7e761", enable_metric=>true, charset=>"UTF-8">, workers=>1, ssl_certificate_verification=>true, ssl_verification_mode=>"full", sniffing=>false, sniffing_delay=>5, timeout=>60, pool_max=>1000, pool_max_per_route=>100, resurrect_delay=>5, validate_after_inactivity=>10000, http_compression=>true, compression_level=>1, retry_initial_interval=>2, retry_max_interval=>64, dlq_on_failed_indexname_interpolation=>true, data_stream_type=>"logs", data_stream_dataset=>"generic", data_stream_namespace=>"default", data_stream_sync_fields=>true, data_stream_auto_routing=>true, manage_template=>true, template_overwrite=>false, template_api=>"auto", doc_as_upsert=>false, script_type=>"inline", script_lang=>"painless", script_var_name=>"event", scripted_upsert=>false, retry_on_conflict=>1, ilm_enabled=>"auto", ilm_pattern=>"{now/d}-000001", ilm_policy=>"logstash-policy">}
[WARN ] 2025-07-28 03:24:31.191 [Converge PipelineAction::Create<main>] elasticsearch - You are using a deprecated config setting "cacert" set in elasticsearch. Deprecated settings will continue to work, but are scheduled for removal from logstash in the future. Set 'ssl_certificate_authorities' instead. If you have any questions about this, please ask it on the https://discuss.elastic.co/c/logstash discussion forum
[WARN ] 2025-07-28 03:24:31.191 [Converge PipelineAction::Create<main>] elasticsearch - You are using a deprecated config setting "ssl" set in elasticsearch. Deprecated settings will continue to work, but are scheduled for removal from logstash in the future. Set 'ssl_enabled' instead. If you have any questions about this, please ask it on the https://discuss.elastic.co/c/logstash discussion forum {:name=>"ssl", :plugin=><LogStash::Outputs::ElasticSearch password=><password>, hosts=>[http://10.211.55.32:9200], cacert=>"/etc/elasticsearch/certs/ca/ca.crt", index=>"logs_for_test_%{+YYYY.MM.dd}", id=>"36015095d477a4f76f2ded08381460cd843ca7cf5f277090662bc7e644f76134", user=>"elastic", ssl=>true, enable_metric=>true, codec=><LogStash::Codecs::Plain id=>"plain_2586a0ce-200a-47db-999e-573711d7e761", enable_metric=>true, charset=>"UTF-8">, workers=>1, ssl_certificate_verification=>true, ssl_verification_mode=>"full", sniffing=>false, sniffing_delay=>5, timeout=>60, pool_max=>1000, pool_max_per_route=>100, resurrect_delay=>5, validate_after_inactivity=>10000, http_compression=>true, compression_level=>1, retry_initial_interval=>2, retry_max_interval=>64, dlq_on_failed_indexname_interpolation=>true, data_stream_type=>"logs", data_stream_dataset=>"generic", data_stream_namespace=>"default", data_stream_sync_fields=>true, data_stream_auto_routing=>true, manage_template=>true, template_overwrite=>false, template_api=>"auto", doc_as_upsert=>false, script_type=>"inline", script_lang=>"painless", script_var_name=>"event", scripted_upsert=>false, retry_on_conflict=>1, ilm_enabled=>"auto", ilm_pattern=>"{now/d}-000001", ilm_policy=>"logstash-policy">}
[WARN ] 2025-07-28 03:24:31.193 [Converge PipelineAction::Create<main>] elasticsearch - You are using a deprecated config setting "ssl" set in elasticsearch. Deprecated settings will continue to work, but are scheduled for removal from logstash in the future. Set 'ssl_enabled' instead. If you have any questions about this, please ask it on the https://discuss.elastic.co/c/logstash discussion forum
[INFO ] 2025-07-28 03:24:31.225 [Converge PipelineAction::Create<main>] javapipeline - Pipeline `main` is configured with `pipeline.ecs_compatibility: v8` setting. All plugins in this pipeline will default to `ecs_compatibility => v8` unless explicitly configured otherwise.
[INFO ] 2025-07-28 03:24:31.238 [[main]-pipeline-manager] elasticsearch - New Elasticsearch output {:class=>"LogStash::Outputs::ElasticSearch", :hosts=>["http://10.211.55.32:9200"]}
[ERROR] 2025-07-28 03:24:31.247 [[main]-pipeline-manager] javapipeline - Pipeline error {:pipeline_id=>"main", :exception=>#<LogStash::ConfigurationError: Explicit value for 'scheme' was declared, but it is different in one of the URLs given! Please make sure your URLs are inline with explicit values. The URLs have the property set to 'http', but it was also set to 'https' explicitly>, :backtrace=>["/usr/share/logstash/vendor/bundle/jruby/3.1.0/gems/logstash-output-elasticsearch-11.22.12-java/lib/logstash/outputs/elasticsearch/http_client.rb:241:in `calculate_property'", "/usr/share/logstash/vendor/bundle/jruby/3.1.0/gems/logstash-output-elasticsearch-11.22.12-java/lib/logstash/outputs/elasticsearch/http_client.rb:272:in `scheme'", "/usr/share/logstash/vendor/bundle/jruby/3.1.0/gems/logstash-output-elasticsearch-11.22.12-java/lib/logstash/outputs/elasticsearch/http_client.rb:72:in `build_url_template'", "/usr/share/logstash/vendor/bundle/jruby/3.1.0/gems/logstash-output-elasticsearch-11.22.12-java/lib/logstash/outputs/elasticsearch/http_client.rb:62:in `initialize'", "org/jruby/RubyClass.java:922:in `new'", "/usr/share/logstash/vendor/bundle/jruby/3.1.0/gems/logstash-output-elasticsearch-11.22.12-java/lib/logstash/outputs/elasticsearch/http_client_builder.rb:106:in `create_http_client'", "/usr/share/logstash/vendor/bundle/jruby/3.1.0/gems/logstash-output-elasticsearch-11.22.12-java/lib/logstash/outputs/elasticsearch/http_client_builder.rb:102:in `build'", "/usr/share/logstash/vendor/bundle/jruby/3.1.0/gems/logstash-output-elasticsearch-11.22.12-java/lib/logstash/plugin_mixins/elasticsearch/common.rb:42:in `build_client'", "/usr/share/logstash/vendor/bundle/jruby/3.1.0/gems/logstash-output-elasticsearch-11.22.12-java/lib/logstash/outputs/elasticsearch.rb:301:in `register'", "org/logstash/config/ir/compiler/AbstractOutputDelegatorExt.java:69:in `register'", "/usr/share/logstash/logstash-core/lib/logstash/java_pipeline.rb:245:in `block in register_plugins'", "org/jruby/RubyArray.java:1981:in `each'", "/usr/share/logstash/logstash-core/lib/logstash/java_pipeline.rb:244:in `register_plugins'", "/usr/share/logstash/logstash-core/lib/logstash/java_pipeline.rb:622:in `maybe_setup_out_plugins'", "/usr/share/logstash/logstash-core/lib/logstash/java_pipeline.rb:257:in `start_workers'", "/usr/share/logstash/logstash-core/lib/logstash/java_pipeline.rb:198:in `run'", "/usr/share/logstash/logstash-core/lib/logstash/java_pipeline.rb:150:in `block in start'"], "pipeline.sources"=>["/etc/logstash/conf.d/filebeat.conf"], :thread=>"#<Thread:0x449f8435 /usr/share/logstash/logstash-core/lib/logstash/java_pipeline.rb:138 run>"}
[INFO ] 2025-07-28 03:24:31.248 [[main]-pipeline-manager] javapipeline - Pipeline terminated {"pipeline.id"=>"main"}
[ERROR] 2025-07-28 03:24:31.251 [Converge PipelineAction::Create<main>] agent - Failed to execute action {:id=>:main, :action_type=>LogStash::ConvergeResult::FailedAction, :message=>"Could not execute action: PipelineAction::Create<main>, action_result: false", :backtrace=>nil}
[INFO ] 2025-07-28 03:24:31.254 [LogStash::Runner] runner - Logstash shut down.
root@elk:/usr/share/logstash/bin#
### WE GOT AN ERROR ABOVE


-----------------------------------------------------------------------------------------------------
TROUBLESHOOTING TIME:
-----------------------------------------------------------------------------------------------------
ERROR:
root@elk:/usr/share/logstash/bin# ./logstash -f /etc/logstash/conf.d/filebeat.conf
...
[ERROR] 2025-07-28 03:24:31.247 [[main]-pipeline-manager] javapipeline - Pipeline error {:pipeline_id=>"main", :exception=>#<LogStash::ConfigurationError: Explicit value for 'scheme' was declared, but it is different in one of the URLs given! Please make sure your URLs are inline with explicit values. The URLs have the property set to 'http', but it was also set to 'https' explicitly>, :backtrace=>["/usr/share/logstash/vendor/bundle/jruby/3.1.0/gems/logstash-output-elasticsearch-11.22.12-java/lib/logstash/outputs/elasticsearch/http_client.rb:241:in `calculate_property'", "/usr/share/logstash/vendor/bundle/jruby/3.1.0/gems/logstash-output-elasticsearch-11.22.12-java/lib/logstash/outputs/elasticsearch/http_client.rb:272:in `scheme'", "/usr/share/logstash/vendor/bundle/jruby/3.1.0/gems/logstash-output-elasticsearch-11.22.12-java/lib/logstash/outputs/elasticsearch/http_client.rb:72:in `build_url_template'", "/usr/share/logstash/vendor/bundle/jruby/3.1.0/gems/logstash-output-elasticsearch-11.22.12-java/lib/logstash/outputs/elasticsearch/http_client.rb:62:in `initialize'", "org/jruby/RubyClass.java:922:in `new'", "/usr/share/logstash/vendor/bundle/jruby/3.1.0/gems/logstash-output-elasticsearch-11.22.12-java/lib/logstash/outputs/elasticsearch/http_client_builder.rb:106:in `create_http_client'", "/usr/share/logstash/vendor/bundle/jruby/3.1.0/gems/logstash-output-elasticsearch-11.22.12-java/lib/logstash/outputs/elasticsearch/http_client_builder.rb:102:in `build'", "/usr/share/logstash/vendor/bundle/jruby/3.1.0/gems/logstash-output-elasticsearch-11.22.12-java/lib/logstash/plugin_mixins/elasticsearch/common.rb:42:in `build_client'", "/usr/share/logstash/vendor/bundle/jruby/3.1.0/gems/logstash-output-elasticsearch-11.22.12-java/lib/logstash/outputs/elasticsearch.rb:301:in `register'", "org/logstash/config/ir/compiler/AbstractOutputDelegatorExt.java:69:in `register'", "/usr/share/logstash/logstash-core/lib/logstash/java_pipeline.rb:245:in `block in register_plugins'", "org/jruby/RubyArray.java:1981:in `each'", "/usr/share/logstash/logstash-core/lib/logstash/java_pipeline.rb:244:in `register_plugins'", "/usr/share/logstash/logstash-core/lib/logstash/java_pipeline.rb:622:in `maybe_setup_out_plugins'", "/usr/share/logstash/logstash-core/lib/logstash/java_pipeline.rb:257:in `start_workers'", "/usr/share/logstash/logstash-core/lib/logstash/java_pipeline.rb:198:in `run'", "/usr/share/logstash/logstash-core/lib/logstash/java_pipeline.rb:150:in `block in start'"], "pipeline.sources"=>["/etc/logstash/conf.d/filebeat.conf"], :thread=>"#<Thread:0x449f8435 /usr/share/logstash/logstash-core/lib/logstash/java_pipeline.rb:138 run>"}
[INFO ] 2025-07-28 03:24:31.248 [[main]-pipeline-manager] javapipeline - Pipeline terminated {"pipeline.id"=>"main"}
[ERROR] 2025-07-28 03:24:31.251 [Converge PipelineAction::Create<main>] agent - Failed to execute action {:id=>:main, :action_type=>LogStash::ConvergeResult::FailedAction, :message=>"Could not execute action: PipelineAction::Create<main>, action_result: false", :backtrace=>nil}
[INFO ] 2025-07-28 03:24:31.254 [LogStash::Runner] runner - Logstash shut down.
root@elk:/usr/share/logstash/bin#


RESOLUTION:
root@elk:/etc/logstash/conf.d# cat filebeat.conf
input {
  beats {
    port => 5044

  }
}

filter {

}

output {
  elasticsearch {
    hosts => ["https://sys.fcc:9200"]  <-------------------------- had to add this, before it was the co-ordinator ip address and protocol was http instead of https
    index => "logs_for_test_%{+YYYY.MM.dd}"
    user => elastic
    password => blfOwgJUMUuLKjmXVYU2
    ssl => true
    cacert => "/etc/elasticsearch/certs/ca/ca.crt"
  }
}

root@elk:/etc/logstash/conf.d#
root@elk:/usr/share/logstash/bin# ./logstash -f /etc/logstash/conf.d/filebeat.conf
Using bundled JDK: /usr/share/logstash/jdk
OpenJDK 64-Bit Server VM warning: Unable to get SVE vector length on this system. Disabling SVE. Specify -XX:UseSVE=0 to shun this warning.
OpenJDK 64-Bit Server VM warning: Unable to get SVE vector length on this system. Disabling SVE. Specify -XX:UseSVE=0 to shun this warning.
WARNING: Could not find logstash.yml which is typically located in $LS_HOME/config or /etc/logstash. You can specify the path using --path.settings. Continuing using the defaults
Could not find log4j2 configuration at path /usr/share/logstash/config/log4j2.properties. Using default config which logs errors to the console
[WARN ] 2025-07-28 03:29:49.802 [main] runner - Starting from version 9.0, running with superuser privileges is not permitted unless you explicitly set 'allow_superuser' to true, thereby acknowledging the possible security risks
[WARN ] 2025-07-28 03:29:49.804 [main] runner - NOTICE: Running Logstash as a superuser is strongly discouraged as it poses a security risk. Set 'allow_superuser' to false for better security.
[WARN ] 2025-07-28 03:29:49.806 [main] runner - 'pipeline.buffer.type' setting is not explicitly defined.Before moving to 9.x set it to 'heap' and tune heap size upward, or set it to 'direct' to maintain existing behavior.
[INFO ] 2025-07-28 03:29:49.806 [main] runner - Starting Logstash {"logstash.version"=>"8.18.4", "jruby.version"=>"jruby 9.4.9.0 (3.1.4) 2024-11-04 547c6b150e OpenJDK 64-Bit Server VM 21.0.7+6-LTS on 21.0.7+6-LTS +indy +jit [aarch64-linux]"}
[INFO ] 2025-07-28 03:29:49.807 [main] runner - JVM bootstrap flags: [-Xms1g, -Xmx1g, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djruby.compile.invokedynamic=true, -XX:+HeapDumpOnOutOfMemoryError, -Djava.security.egd=file:/dev/urandom, -Dlog4j2.isThreadContextMapInheritable=true, -Djruby.regexp.interruptible=true, -Djdk.io.File.enableADS=true, --add-exports=jdk.compiler/com.sun.tools.javac.api=ALL-UNNAMED, --add-exports=jdk.compiler/com.sun.tools.javac.file=ALL-UNNAMED, --add-exports=jdk.compiler/com.sun.tools.javac.parser=ALL-UNNAMED, --add-exports=jdk.compiler/com.sun.tools.javac.tree=ALL-UNNAMED, --add-exports=jdk.compiler/com.sun.tools.javac.util=ALL-UNNAMED, --add-opens=java.base/java.security=ALL-UNNAMED, --add-opens=java.base/java.io=ALL-UNNAMED, --add-opens=java.base/java.nio.channels=ALL-UNNAMED, --add-opens=java.base/sun.nio.ch=ALL-UNNAMED, --add-opens=java.management/sun.management=ALL-UNNAMED, -Dio.netty.allocator.maxOrder=11]
[INFO ] 2025-07-28 03:29:49.855 [main] StreamReadConstraintsUtil - Jackson default value override `logstash.jackson.stream-read-constraints.max-string-length` configured to `200000000` (logstash default)
[INFO ] 2025-07-28 03:29:49.855 [main] StreamReadConstraintsUtil - Jackson default value override `logstash.jackson.stream-read-constraints.max-number-length` configured to `10000` (logstash default)
[INFO ] 2025-07-28 03:29:49.855 [main] StreamReadConstraintsUtil - Jackson default value override `logstash.jackson.stream-read-constraints.max-nesting-depth` configured to `1000` (logstash default)
[WARN ] 2025-07-28 03:29:49.910 [LogStash::Runner] multilocal - Ignoring the 'pipelines.yml' file because modules or command line options are specified
[INFO ] 2025-07-28 03:29:50.087 [Api Webserver] agent - Successfully started Logstash API endpoint {:port=>9600, :ssl_enabled=>false}
[INFO ] 2025-07-28 03:29:50.240 [Converge PipelineAction::Create<main>] Reflections - Reflections took 82 ms to scan 1 urls, producing 150 keys and 529 values
[WARN ] 2025-07-28 03:29:50.336 [Converge PipelineAction::Create<main>] elasticsearch - You are using a deprecated config setting "cacert" set in elasticsearch. Deprecated settings will continue to work, but are scheduled for removal from logstash in the future. Set 'ssl_certificate_authorities' instead. If you have any questions about this, please ask it on the https://discuss.elastic.co/c/logstash discussion forum {:name=>"cacert", :plugin=><LogStash::Outputs::ElasticSearch password=><password>, hosts=>[https://sys.fcc:9200], cacert=>"/etc/elasticsearch/certs/ca/ca.crt", index=>"logs_for_test_%{+YYYY.MM.dd}", id=>"f80ef03d811c5f54c018a0dd5d173e4455516dce1da7395d07817ad7bc58db70", user=>"elastic", ssl=>true, enable_metric=>true, codec=><LogStash::Codecs::Plain id=>"plain_e286e811-aab4-4175-80f3-9c8f254b2150", enable_metric=>true, charset=>"UTF-8">, workers=>1, ssl_certificate_verification=>true, ssl_verification_mode=>"full", sniffing=>false, sniffing_delay=>5, timeout=>60, pool_max=>1000, pool_max_per_route=>100, resurrect_delay=>5, validate_after_inactivity=>10000, http_compression=>true, compression_level=>1, retry_initial_interval=>2, retry_max_interval=>64, dlq_on_failed_indexname_interpolation=>true, data_stream_type=>"logs", data_stream_dataset=>"generic", data_stream_namespace=>"default", data_stream_sync_fields=>true, data_stream_auto_routing=>true, manage_template=>true, template_overwrite=>false, template_api=>"auto", doc_as_upsert=>false, script_type=>"inline", script_lang=>"painless", script_var_name=>"event", scripted_upsert=>false, retry_on_conflict=>1, ilm_enabled=>"auto", ilm_pattern=>"{now/d}-000001", ilm_policy=>"logstash-policy">}
[WARN ] 2025-07-28 03:29:50.336 [Converge PipelineAction::Create<main>] elasticsearch - You are using a deprecated config setting "cacert" set in elasticsearch. Deprecated settings will continue to work, but are scheduled for removal from logstash in the future. Set 'ssl_certificate_authorities' instead. If you have any questions about this, please ask it on the https://discuss.elastic.co/c/logstash discussion forum
[WARN ] 2025-07-28 03:29:50.336 [Converge PipelineAction::Create<main>] elasticsearch - You are using a deprecated config setting "ssl" set in elasticsearch. Deprecated settings will continue to work, but are scheduled for removal from logstash in the future. Set 'ssl_enabled' instead. If you have any questions about this, please ask it on the https://discuss.elastic.co/c/logstash discussion forum {:name=>"ssl", :plugin=><LogStash::Outputs::ElasticSearch password=><password>, hosts=>[https://sys.fcc:9200], cacert=>"/etc/elasticsearch/certs/ca/ca.crt", index=>"logs_for_test_%{+YYYY.MM.dd}", id=>"f80ef03d811c5f54c018a0dd5d173e4455516dce1da7395d07817ad7bc58db70", user=>"elastic", ssl=>true, enable_metric=>true, codec=><LogStash::Codecs::Plain id=>"plain_e286e811-aab4-4175-80f3-9c8f254b2150", enable_metric=>true, charset=>"UTF-8">, workers=>1, ssl_certificate_verification=>true, ssl_verification_mode=>"full", sniffing=>false, sniffing_delay=>5, timeout=>60, pool_max=>1000, pool_max_per_route=>100, resurrect_delay=>5, validate_after_inactivity=>10000, http_compression=>true, compression_level=>1, retry_initial_interval=>2, retry_max_interval=>64, dlq_on_failed_indexname_interpolation=>true, data_stream_type=>"logs", data_stream_dataset=>"generic", data_stream_namespace=>"default", data_stream_sync_fields=>true, data_stream_auto_routing=>true, manage_template=>true, template_overwrite=>false, template_api=>"auto", doc_as_upsert=>false, script_type=>"inline", script_lang=>"painless", script_var_name=>"event", scripted_upsert=>false, retry_on_conflict=>1, ilm_enabled=>"auto", ilm_pattern=>"{now/d}-000001", ilm_policy=>"logstash-policy">}
[WARN ] 2025-07-28 03:29:50.337 [Converge PipelineAction::Create<main>] elasticsearch - You are using a deprecated config setting "ssl" set in elasticsearch. Deprecated settings will continue to work, but are scheduled for removal from logstash in the future. Set 'ssl_enabled' instead. If you have any questions about this, please ask it on the https://discuss.elastic.co/c/logstash discussion forum
[INFO ] 2025-07-28 03:29:50.344 [Converge PipelineAction::Create<main>] javapipeline - Pipeline `main` is configured with `pipeline.ecs_compatibility: v8` setting. All plugins in this pipeline will default to `ecs_compatibility => v8` unless explicitly configured otherwise.
[INFO ] 2025-07-28 03:29:50.349 [[main]-pipeline-manager] elasticsearch - New Elasticsearch output {:class=>"LogStash::Outputs::ElasticSearch", :hosts=>["https://sys.fcc:9200"]}
[INFO ] 2025-07-28 03:29:50.392 [[main]-pipeline-manager] elasticsearch - Elasticsearch pool URLs updated {:changes=>{:removed=>[], :added=>[https://elastic:xxxxxx@sys.fcc:9200/]}}
[WARN ] 2025-07-28 03:29:50.604 [[main]-pipeline-manager] elasticsearch - Restored connection to ES instance {:url=>"https://elastic:xxxxxx@sys.fcc:9200/"}
[INFO ] 2025-07-28 03:29:50.604 [[main]-pipeline-manager] elasticsearch - Elasticsearch version determined (8.18.1) {:es_version=>8}
[INFO ] 2025-07-28 03:29:50.635 [[main]-pipeline-manager] elasticsearch - Not eligible for data streams because config contains one or more settings that are not compatible with data streams: {"index"=>"logs_for_test_%{+YYYY.MM.dd}"}
[INFO ] 2025-07-28 03:29:50.635 [[main]-pipeline-manager] elasticsearch - Data streams auto configuration (`data_stream => auto` or unset) resolved to `false`
[INFO ] 2025-07-28 03:29:50.645 [Ruby-0-Thread-10: /usr/share/logstash/vendor/bundle/jruby/3.1.0/gems/logstash-output-elasticsearch-11.22.12-java/lib/logstash/plugin_mixins/elasticsearch/common.rb:164] elasticsearch - Using a default mapping template {:es_version=>8, :ecs_compatibility=>:v8}
[INFO ] 2025-07-28 03:29:50.646 [[main]-pipeline-manager] javapipeline - Starting pipeline {:pipeline_id=>"main", "pipeline.workers"=>2, "pipeline.batch.size"=>125, "pipeline.batch.delay"=>50, "pipeline.max_inflight"=>250, "pipeline.sources"=>["/etc/logstash/conf.d/filebeat.conf"], :thread=>"#<Thread:0x5e80f42c /usr/share/logstash/logstash-core/lib/logstash/java_pipeline.rb:138 run>"}
[INFO ] 2025-07-28 03:29:50.696 [Ruby-0-Thread-10: /usr/share/logstash/vendor/bundle/jruby/3.1.0/gems/logstash-output-elasticsearch-11.22.12-java/lib/logstash/plugin_mixins/elasticsearch/common.rb:164] elasticsearch - Installing Elasticsearch template {:name=>"ecs-logstash"}
[INFO ] 2025-07-28 03:29:50.925 [[main]-pipeline-manager] javapipeline - Pipeline Java execution initialization time {"seconds"=>0.28}
[INFO ] 2025-07-28 03:29:50.927 [[main]-pipeline-manager] beats - Starting input listener {:address=>"0.0.0.0:5044"}
[INFO ] 2025-07-28 03:29:50.933 [[main]-pipeline-manager] javapipeline - Pipeline started {"pipeline.id"=>"main"}
[INFO ] 2025-07-28 03:29:50.936 [Agent thread] agent - Pipelines running {:count=>1, :running_pipelines=>[:main], :non_running_pipelines=>[]}
[INFO ] 2025-07-28 03:29:50.971 [[main]<beats] Server - Starting server on port: 5044
-----------------------------------------------------------------------------------------------------
TROUBLESHOOTING TIME ENDS:
-----------------------------------------------------------------------------------------------------

Important:
note: that you will have to start logstash first and then filebeat. Because the Logstash is a downstream. Filebeat is going to write data do the logstash, so the logstash should be up and running before the filebeat. 
## another reason for running the logstash first is so it can recieve logs and port for logstash por 5044 gets open when the logstash start running and that is the port on which filebeat sends it's logs to logstash. 
# ON NODE-6
root@elk:/usr/share/elasticsearch/bin# systemctl status filebeat
○ filebeat.service - Filebeat sends log files to Logstash or directly to Elasticsearch.
     Loaded: loaded (/usr/lib/systemd/system/filebeat.service; enabled; preset: enabled)
     Active: inactive (dead) since Mon 2025-07-28 03:57:10 UTC; 7s ago
   Duration: 20h 59min 46.738s
       Docs: https://www.elastic.co/beats/filebeat
    Process: 65534 ExecStart=/usr/share/filebeat/bin/filebeat --environment systemd $BEAT_LOG_OPTS $BEAT_CONFIG_OPTS $BEAT_PATH_OPTS (code=exited, status=0/SUCCESS)
   Main PID: 65534 (code=exited, status=0/SUCCESS)
        CPU: 29.980s

------------------------------------------------------------------------
NOW WE HAVE TO CONFIGURE FILEBEAT ON NODE-6
------------------------------------------------------------------------------------

root@elk:/etc/filebeat# cat filebeat.yml
###################### Filebeat Configuration Example #########################

# This file is an example configuration file highlighting only the most common
# options. The filebeat.reference.yml file from the same directory contains all the
# supported options with more comments. You can use it as a reference.
#
# You can find the full configuration reference here:
# https://www.elastic.co/guide/en/beats/filebeat/index.html

# For more available modules and options, please see the filebeat.reference.yml sample
# configuration file.

# ============================== Filebeat inputs ===============================

filebeat.inputs:

# Each - is an input. Most options can be set at the input level, so
# you can use different inputs for various configurations.
# Below are the input-specific configurations.

# filestream is an input for collecting log messages from files.
- type: filestream

  # Unique ID among all inputs, an ID is required.
  id: my-filestream-id

  # Change to true to enable this input configuration.
  enabled: false

  # Paths that should be crawled and fetched. Glob based paths.
  paths:
    - /var/log/*.log    <------------------------------------------------------------------------------------------ ### filebeat is reading the logs from this file path
    #- c:\programdata\elasticsearch\logs\*

  # Exclude lines. A list of regular expressions to match. It drops the lines that are
  # matching any regular expression from the list.
  # Line filtering happens after the parsers pipeline. If you would like to filter lines
  # before parsers, use include_message parser.
  #exclude_lines: ['^DBG']

  # Include lines. A list of regular expressions to match. It exports the lines that are
  # matching any regular expression from the list.
  # Line filtering happens after the parsers pipeline. If you would like to filter lines
  # before parsers, use include_message parser.
  #include_lines: ['^ERR', '^WARN']

  # Exclude files. A list of regular expressions to match. Filebeat drops the files that
  # are matching any regular expression from the list. By default, no files are dropped.
  #prospector.scanner.exclude_files: ['.gz$']

  # Optional additional fields. These fields can be freely picked
  # to add additional information to the crawled log files for filtering
  #fields:
  #  level: debug
  #  review: 1

# journald is an input for collecting logs from Journald
#- type: journald

  # Unique ID among all inputs, if the ID changes, all entries
  # will be re-ingested
  #id: my-journald-id

  # The position to start reading from the journal, valid options are:
  #  - head: Starts reading at the beginning of the journal.
  #  - tail: Starts reading at the end of the journal.
  #    This means that no events will be sent until a new message is written.
  #  - since: Use also the `since` option to determine when to start reading from.
  #seek: head

  # A time offset from the current time to start reading from.
  # To use since, seek option must be set to since.
  #since: -24h

  # Collect events from the service and messages about the service,
  # including coredumps.
  #units:
    #- docker.service

# ============================== Filebeat modules ==============================

filebeat.config.modules:
  # Glob pattern for configuration loading
  path: ${path.config}/modules.d/*.yml

  # Set to true to enable config reloading
  reload.enabled: false

  # Period on which files under path should be checked for changes
  #reload.period: 10s

# ======================= Elasticsearch template setting =======================

setup.template.settings:
  index.number_of_shards: 1
  #index.codec: best_compression
  #_source.enabled: false


# ================================== General ===================================

# The name of the shipper that publishes the network data. It can be used to group
# all the transactions sent by a single shipper in the web interface.
#name:

# The tags of the shipper are included in their field with each
# transaction published.
#tags: ["service-X", "web-tier"]

# Optional fields that you can specify to add additional information to the
# output.
#fields:
#  env: staging

# ================================= Dashboards =================================
# These settings control loading the sample dashboards to the Kibana index. Loading
# the dashboards is disabled by default and can be enabled either by setting the
# options here or by using the `setup` command.
#setup.dashboards.enabled: false

# The URL from where to download the dashboard archive. By default, this URL
# has a value that is computed based on the Beat name and version. For released
# versions, this URL points to the dashboard archive on the artifacts.elastic.co
# website.
#setup.dashboards.url:

# =================================== Kibana ===================================

# Starting with Beats version 6.0.0, the dashboards are loaded via the Kibana API.
# This requires a Kibana endpoint configuration.
setup.kibana:

  # Kibana Host
  # Scheme and port can be left out and will be set to the default (http and 5601)
  # In case you specify and additional path, the scheme is required: http://localhost:5601/path
  # IPv6 addresses should always be defined as: https://[2001:db8::1]:5601
  #host: "localhost:5601"

  # Kibana Space ID
  # ID of the Kibana Space into which the dashboards should be loaded. By default,
  # the Default Space will be used.
  #space.id:

# =============================== Elastic Cloud ================================

# These settings simplify using Filebeat with the Elastic Cloud (https://cloud.elastic.co/).

# The cloud.id setting overwrites the `output.elasticsearch.hosts` and
# `setup.kibana.host` options.
# You can find the `cloud.id` in the Elastic Cloud web UI.
#cloud.id:

# The cloud.auth setting overwrites the `output.elasticsearch.username` and
# `output.elasticsearch.password` settings. The format is `<user>:<pass>`.
#cloud.auth:

# ================================== Outputs ===================================

# Configure what output to use when sending the data collected by the beat.

# ---------------------------- Elasticsearch Output ----------------------------
output.elasticsearch:
  # Array of hosts to connect to.
  hosts: ["localhost:9200"]

  # Performance preset - one of "balanced", "throughput", "scale",
  # "latency", or "custom".
  preset: balanced

  # Protocol - either `http` (default) or `https`.
  #protocol: "https"

  # Authentication credentials - either API key or username/password.
  #api_key: "id:api_key"
  #username: "elastic"
  #password: "changeme"

# ------------------------------ Logstash Output -------------------------------
#output.logstash:
  # The Logstash hosts
  #hosts: ["localhost:5044"]

  # Optional SSL. By default is off.
  # List of root certificates for HTTPS server verifications
  #ssl.certificate_authorities: ["/etc/pki/root/ca.pem"]

  # Certificate for SSL client authentication
  #ssl.certificate: "/etc/pki/client/cert.pem"

  # Client Certificate Key
  #ssl.key: "/etc/pki/client/cert.key"

# ================================= Processors =================================
processors:
  - add_host_metadata:
      when.not.contains.tags: forwarded
  - add_cloud_metadata: ~
  - add_docker_metadata: ~
  - add_kubernetes_metadata: ~

# ================================== Logging ===================================

# Sets log level. The default log level is info.
# Available log levels are: error, warning, info, debug
#logging.level: debug

# At debug level, you can selectively enable logging only for some components.
# To enable all selectors, use ["*"]. Examples of other selectors are "beat",
# "publisher", "service".
#logging.selectors: ["*"]

# ============================= X-Pack Monitoring ==============================
# Filebeat can export internal metrics to a central Elasticsearch monitoring
# cluster.  This requires xpack monitoring to be enabled in Elasticsearch.  The
# reporting is disabled by default.

# Set to true to enable the monitoring reporter.
#monitoring.enabled: false

# Sets the UUID of the Elasticsearch cluster under which monitoring data for this
# Filebeat instance will appear in the Stack Monitoring UI. If output.elasticsearch
# is enabled, the UUID is derived from the Elasticsearch cluster referenced by output.elasticsearch.
#monitoring.cluster_uuid:

# Uncomment to send the metrics to Elasticsearch. Most settings from the
# Elasticsearch outputs are accepted here as well.
# Note that the settings should point to your Elasticsearch *monitoring* cluster.
# Any setting that is not set is automatically inherited from the Elasticsearch
# output configuration, so if you have the Elasticsearch output configured such
# that it is pointing to your Elasticsearch monitoring cluster, you can simply
# uncomment the following line.
#monitoring.elasticsearch:

# ============================== Instrumentation ===============================

# Instrumentation support for the filebeat.
#instrumentation:
    # Set to true to enable instrumentation of filebeat.
    #enabled: false

    # Environment in which filebeat is running on (eg: staging, production, etc.)
    #environment: ""

    # APM Server hosts to report instrumentation results to.
    #hosts:
    #  - http://localhost:8200

    # API Key for the APM Server(s).
    # If api_key is set then secret_token will be ignored.
    #api_key:

    # Secret token for the APM Server(s).
    #secret_token:


# ================================= Migration ==================================

# This allows to enable 6.7 migration aliases
#migration.6_to_7.enabled: true

root@elk:/etc/filebeat#

root@elk:/etc/filebeat# cd /var/log/
root@elk:/var/log# ll
total 800
drwxr-xr-x   9 root          root              4096 Jun  1 20:29 ./
drwxr-xr-x  13 root          root              4096 May 31 04:32 ../
lrwxrwxrwx   1 root          root                39 Feb 16 21:00 README -> ../../usr/share/doc/systemd/README.logs
-rw-r--r--   1 root          root             25528 Jun  1 20:40 alternatives.log
-rw-r-----   1 root          adm                  0 May 31 04:32 apport.log
drwxr-xr-x   2 root          root              4096 Jul 27 22:46 apt/
-rw-r--r--   1 root          root             61237 Feb 16 20:53 bootstrap.log
-rw-rw----   1 root          utmp                 0 Feb 16 20:53 btmp
-rw-r-----   1 root          adm               4192 May 31 04:32 cloud-init-output.log
-rw-r-----   1 root          adm              71446 May 31 04:32 cloud-init.log
drwxr-xr-x   2 root          root              4096 Jan 31 17:11 dist-upgrade/
-rw-r--r--   1 root          root            576219 Jul 27 22:46 dpkg.log
drwxr-s---   2 elasticsearch elasticsearch    12288 Jul 28 03:12 elasticsearch/
-rw-r--r--   1 root          root                 0 Feb 16 20:53 faillog
-rw-r--r--   1 root          root               606 Jun  1 20:29 fontconfig.log
drwxrwx---   4 root          adm               4096 May 31 04:10 installer/
drwxr-sr-x+  3 root          systemd-journal   4096 May 31 04:32 journal/
-rw-rw-r--   1 root          utmp            296296 Jul 17 01:29 lastlog
drwx------   2 root          root              4096 Feb 16 21:00 private/
drwxr-x---   2 root          adm               4096 May 31 04:32 unattended-upgrades/
-rw-rw-r--   1 root          utmp              7600 Jul 17 01:29 wtmp
root@elk:/var/log#

### continuing on commenting and uncommenting more lines in the filebeat.yml file
### currently we didnt specify any filters or anything, we are just going to enable it
### later we will do the customization
root@elk:/etc/filebeat# vim filebeat.yml
...
# ============================== Filebeat inputs ===============================

filebeat.inputs:

# Each - is an input. Most options can be set at the input level, so
# you can use different inputs for various configurations.
# Below are the input-specific configurations.

# filestream is an input for collecting log messages from files.
- type: filestream

  # Unique ID among all inputs, an ID is required.
  id: my-filestream-id

  # Change to true to enable this input configuration.
  enabled: true    <---------------------------------------------------------------------------------------- # enabled

  # Paths that should be crawled and fetched. Glob based paths.
  paths:
    - /var/log/*.log
    #- c:\programdata\elasticsearch\logs\*
...
### we will comment few lines from Elastic Cloud section of the filebeat.yml file, because we arent writing the logs to the elasticsearch
# =============================== Elastic Cloud ================================

# These settings simplify using Filebeat with the Elastic Cloud (https://cloud.elastic.co/).

# The cloud.id setting overwrites the `output.elasticsearch.hosts` and
# `setup.kibana.host` options.
# You can find the `cloud.id` in the Elastic Cloud web UI.
#cloud.id:

# The cloud.auth setting overwrites the `output.elasticsearch.username` and
# `output.elasticsearch.password` settings. The format is `<user>:<pass>`.
#cloud.auth:

# ================================== Outputs ===================================

# Configure what output to use when sending the data collected by the beat.

# ---------------------------- Elasticsearch Output ----------------------------
#output.elasticsearch: <---------------------------------------------------------------------- ### we commented out
  # Array of hosts to connect to.
  #  hosts: ["localhost:9200"] <---------------------------------------------------------------------- ### we commented out 

  # Performance preset - one of "balanced", "throughput", "scale",
  # "latency", or "custom".
  # preset: balanced <---------------------------------------------------------------------- ### we commented out

  # Protocol - either `http` (default) or `https`.
  #protocol: "https"

  # Authentication credentials - either API key or username/password.
  #api_key: "id:api_key"
  #username: "elastic"
  #password: "changeme"
...
### in this section of the filebeat.yml file, we will enable the logstash
# ------------------------------ Logstash Output -------------------------------
output.logstash:  <-------------------------------------------------------------------------- ### uncomment
  # The Logstash hosts
  hosts: ["10.211.55.31:5044"] <-------------------------------------------------------------------------- ### uncomment

  # Optional SSL. By default is off.
  # List of root certificates for HTTPS server verifications
  #ssl.certificate_authorities: ["/etc/pki/root/ca.pem"]

  # Certificate for SSL client authentication
  #ssl.certificate: "/etc/pki/client/cert.pem"

  # Client Certificate Key
  #ssl.key: "/etc/pki/client/cert.key"
...
:wq!


root@elk:/usr/share/logstash/bin# systemctl start logstash
root@elk:/usr/share/logstash/bin# systemctl enable logstash
Created symlink /etc/systemd/system/multi-user.target.wants/logstash.service → /usr/lib/systemd/system/logstash.service.
root@elk:/usr/share/logstash/bin# systemctl status logstash
● logstash.service - logstash
     Loaded: loaded (/usr/lib/systemd/system/logstash.service; enabled; preset: enabled)
     Active: active (running) since Mon 2025-07-28 04:41:29 UTC; 3s ago
   Main PID: 138859 (java)
      Tasks: 21 (limit: 4549)
     Memory: 357.3M (peak: 357.5M)
        CPU: 6.017s
     CGroup: /system.slice/logstash.service
             └─138859 /usr/share/logstash/jdk/bin/java -Xms1g -Xmx1g -Djava.awt.headless=true -Dfile.encoding=UTF-8 -Djruby.compile.invokedynamic=true -XX:+HeapDumpOnOutOfMemoryError -Djava.security.egd=file:/dev/urandom -Dlog4j2.isThreadCon>

Jul 28 04:41:29 elk systemd[1]: logstash.service: Scheduled restart job, restart counter is at 5.
Jul 28 04:41:29 elk systemd[1]: Started logstash.service - logstash.
Jul 28 04:41:29 elk logstash[138859]: Using bundled JDK: /usr/share/logstash/jdk
Jul 28 04:41:29 elk logstash[138868]: OpenJDK 64-Bit Server VM warning: Unable to get SVE vector length on this system. Disabling SVE. Specify -XX:UseSVE=0 to shun this warning.
Jul 28 04:41:29 elk logstash[138859]: OpenJDK 64-Bit Server

-----------------------------------------------------------------------
### we are going to see combination of logstash and filebeat to send a log to the elasticsearch. We will see how this communication will happen in between them. 

### we will check on node-5 whether the service is running or not. Now we have to that the port through which this config file we opened is working or not
root@elk:~# more /etc/logstash/conf.d/filebeat.conf
input {
  beats {
    port => 5044

  }
}

filter {

}

output {
  elasticsearch {
    hosts => ["https://sys.fcc:9200"]
    index => "logs_for_test_%{+YYYY.MM.dd}"
    user => elastic
    password => blfOwgJUMUuLKjmXVYU2
    ssl => true
    cacert => "/etc/elasticsearch/certs/ca/ca.crt"
  }
}

### not able to make connection with the port 5044
root@elk:~# telnet localhost 5044
Trying 127.0.0.1...
telnet: Unable to connect to remote host: Connection refused

--------------------------------------------------------------------------------
TROUBLESHOOTING TIME:
--------------------------------------------------------------------------------
ERROR:
### cant reach the port on 5044
root@elk:~# telnet localhost 5044
Trying 127.0.0.1...
telnet: Unable to connect to remote host: Connection refused

ANALYSIS:
  ### ERROR FOUND IN LOGSTASH LOGS
root@elk:/var/log/logstash# tail logstash-plain.log
      ...
    }
  }
----> [2025-07-29T00:57:08,006][ERROR][logstash.agent           ] Failed to execute action {:action=>LogStash::PipelineAction::Create/pipeline_id:main, :exception=>"Java::JavaLang::IllegalStateException", :message=>"Unable to configure plugins: (ConfigurationError) Something is wrong with your configuration.", :backtrace=>["org.logstash.config.ir.CompiledPipeline.<init>(CompiledPipeline.java:137)", "org.logstash.execution.AbstractPipelineExt.initialize(AbstractPipelineExt.java:236)", "org.logstash.execution.AbstractPipelineExt$INVOKER$i$initialize.call(AbstractPipelineExt$INVOKER$i$initialize.gen)", "org.jruby.internal.runtime.methods.JavaMethod$JavaMethodN.call(JavaMethod.java:847)", "org.jruby.ir.runtime.IRRuntimeHelpers.instanceSuper(IRRuntimeHelpers.java:1379)", "org.jruby.ir.instructions.InstanceSuperInstr.interpret(InstanceSuperInstr.java:139)", "org.jruby.ir.interpreter.InterpreterEngine.processCall(InterpreterEngine.java:363)", "org.jruby.ir.interpreter.StartupInterpreterEngine.interpret(StartupInterpreterEngine.java:66)", "org.jruby.internal.runtime.methods.MixedModeIRMethod.INTERPRET_METHOD(MixedModeIRMethod.java:128)", "org.jruby.internal.runtime.methods.MixedModeIRMethod.call(MixedModeIRMethod.java:115)", "org.jruby.runtime.callsite.CachingCallSite.cacheAndCall(CachingCallSite.java:446)", "org.jruby.runtime.callsite.CachingCallSite.call(CachingCallSite.java:92)", "org.jruby.RubyClass.newInstance(RubyClass.java:949)", "org.jruby.RubyClass$INVOKER$i$newInstance.call(RubyClass$INVOKER$i$newInstance.gen)", "org.jruby.runtime.callsite.CachingCallSite.cacheAndCall(CachingCallSite.java:446)", "org.jruby.runtime.callsite.CachingCallSite.call(CachingCallSite.java:92)", "org.jruby.ir.instructions.CallBase.interpret(CallBase.java:548)", "org.jruby.ir.interpreter.InterpreterEngine.processCall(InterpreterEngine.java:363)", "org.jruby.ir.interpreter.StartupInterpreterEngine.interpret(StartupInterpreterEngine.java:66)", "org.jruby.ir.interpreter.InterpreterEngine.interpret(InterpreterEngine.java:88)", "org.jruby.internal.runtime.methods.MixedModeIRMethod.INTERPRET_METHOD(MixedModeIRMethod.java:238)", "org.jruby.internal.runtime.methods.MixedModeIRMethod.call(MixedModeIRMethod.java:225)", "org.jruby.internal.runtime.methods.DynamicMethod.call(DynamicMethod.java:228)", "org.jruby.runtime.callsite.CachingCallSite.cacheAndCall(CachingCallSite.java:476)", "org.jruby.runtime.callsite.CachingCallSite.call(CachingCallSite.java:293)", "org.jruby.ir.interpreter.InterpreterEngine.processCall(InterpreterEngine.java:324)", "org.jruby.ir.interpreter.StartupInterpreterEngine.interpret(StartupInterpreterEngine.java:66)", "org.jruby.ir.interpreter.Interpreter.INTERPRET_BLOCK(Interpreter.java:118)", "org.jruby.runtime.MixedModeIRBlockBody.commonYieldPath(MixedModeIRBlockBody.java:136)", "org.jruby.runtime.IRBlockBody.call(IRBlockBody.java:66)", "org.jruby.runtime.IRBlockBody.call(IRBlockBody.java:58)", "org.jruby.runtime.Block.call(Block.java:144)", "org.jruby.RubyProc.call(RubyProc.java:354)", "org.jruby.internal.runtime.RubyRunnable.run(RubyRunnable.java:111)", "java.base/java.lang.Thread.run(Thread.java:1583)"], :cause=>{:exception=>Java::OrgJrubyExceptions::Exception, :message=>"(ConfigurationError) Something is wrong with your configuration.", :backtrace=>["RUBY.config_init(/usr/share/logstash/logstash-core/lib/logstash/config/mixin.rb:111)", "RUBY.config_init(/usr/share/logstash/vendor/bundle/jruby/3.1.0/gems/logstash-output-elasticsearch-11.22.12-java/lib/logstash/outputs/elasticsearch.rb:390)", "RUBY.initialize(/usr/share/logstash/logstash-core/lib/logstash/outputs/base.rb:75)", "RUBY.initialize(/usr/share/logstash/vendor/bundle/jruby/3.1.0/gems/logstash-mixin-ecs_compatibility_support-1.3.0-java/lib/logstash/plugin_mixins/ecs_compatibility_support/selector.rb:61)", "RUBY.initialize(/usr/share/logstash/vendor/bundle/jruby/3.1.0/gems/logstash-output-elasticsearch-11.22.12-java/lib/logstash/outputs/elasticsearch.rb:276)", "org.logstash.plugins.factory.ContextualizerExt.initialize(org/logstash/plugins/factory/ContextualizerExt.java:97)", "org.jruby.RubyClass.new(org/jruby/RubyClass.java:949)", "org.logstash.plugins.factory.ContextualizerExt.initialize_plugin(org/logstash/plugins/factory/ContextualizerExt.java:80)", "org.logstash.plugins.factory.ContextualizerExt.initialize_plugin(org/logstash/plugins/factory/ContextualizerExt.java:53)", "org.jruby.RubyClass.new(org/jruby/RubyClass.java:949)", "org.logstash.config.ir.compiler.OutputDelegatorExt.initialize(org/logstash/config/ir/compiler/OutputDelegatorExt.java:79)", "org.logstash.config.ir.compiler.OutputDelegatorExt.initialize(org/logstash/config/ir/compiler/OutputDelegatorExt.java:56)", "org.logstash.plugins.factory.PluginFactoryExt.plugin(org/logstash/plugins/factory/PluginFactoryExt.java:241)", "org.logstash.execution.AbstractPipelineExt.initialize(org/logstash/execution/AbstractPipelineExt.java:236)", "RUBY.initialize(/usr/share/logstash/logstash-core/lib/logstash/java_pipeline.rb:47)", "org.jruby.RubyClass.new(org/jruby/RubyClass.java:949)", "RUBY.execute(/usr/share/logstash/logstash-core/lib/logstash/pipeline_action/create.rb:50)", "RUBY.converge_state(/usr/share/logstash/logstash-core/lib/logstash/agent.rb:431)"]}}
[2025-07-29T00:57:08,015][INFO ][logstash.runner          ] Logstash shut down.
[2025-07-29T00:57:08,017][FATAL][org.logstash.Logstash    ] Logstash stopped processing because of an error: (SystemExit) exit
org.jruby.exceptions.SystemExit: (SystemExit) exit
	at org.jruby.RubyKernel.exit(org/jruby/RubyKernel.java:924) ~[jruby.jar:?]
	at org.jruby.RubyKernel.exit(org/jruby/RubyKernel.java:883) ~[jruby.jar:?]
	at usr.share.logstash.lib.bootstrap.environment.<main>(/usr/share/logstash/lib/bootstrap/environment.rb:90) ~[?:?]
root@elk:/var/log/logstash#

  ### CHECKING CONFIGURATION 
root@elk:/var/log/logstash# cd /etc/logstash/conf.d/
root@elk:/etc/logstash/conf.d# vim filebeat.conf
input {
  beats {
    port => 5044

  }
}

filter {

}

output {
  elasticsearch {
    hosts => ["https://sys.fcc:9200"]
    index => "logs_for_test_%{+YYYY.MM.dd}"
    user => elastic
    password => blfOwgJUMUuLKjmXVYU2
    ssl => true
    cacert => "/etc/elasticsearch/certs/ca/ca.crt"
  }
}
:q!

  ### WE ARE TRYING TO RUN LOGSTASH MANUALLY TO SEE WHAT IS THE ISSUE 
root@elk:/etc/logstash/conf.d# systemctl stop logstash
root@elk:/etc/logstash/conf.d# /usr/share/logstash/bin/logstash -f /etc/logstash/conf.d/filebeat.conf
...
ping template {:es_version=>8, :ecs_compatibility=>:v8}
[INFO ] 2025-07-29 01:05:49.746 [[main]-pipeline-manager] javapipeline - Starting pipeline {:pipeline_id=>"main", "pipeline.workers"=>2, "pipeline.batch.size"=>125, "pipeline.batch.delay"=>50, "pipeline.max_inflight"=>250, "pipeline.sources"=>["/etc/logstash/conf.d/filebeat.conf"], :thread=>"#<Thread:0x9dd58e6 /usr/share/logstash/logstash-core/lib/logstash/java_pipeline.rb:138 run>"}
[INFO ] 2025-07-29 01:05:49.921 [[main]-pipeline-manager] javapipeline - Pipeline Java execution initialization time {"seconds"=>0.17}
[INFO ] 2025-07-29 01:05:49.923 [[main]-pipeline-manager] beats - Starting input listener {:address=>"0.0.0.0:5044"}
[INFO ] 2025-07-29 01:05:49.926 [[main]-pipeline-manager] javapipeline - Pipeline started {"pipeline.id"=>"main"}
[INFO ] 2025-07-29 01:05:49.933 [Agent thread] agent - Pipelines running {:count=>1, :running_pipelines=>[:main], :non_running_pipelines=>[]}
[INFO ] 2025-07-29 01:05:49.953 [[main]<beats] Server - Starting server on port: 5044 ### <----------------------- ### when we are running manually, the pipleine is running and connecting 

  ### manual pipeline is working, so we will troubleshoot to see what error we get while RUNNING AS A DAEMON.
  ### On node-6 we will start the filebeat, meanwhile the logstash is running manually on Node-5
root@elk:/etc/filebeat# vim filebeat.yml
root@elk:/etc/filebeat# systemctl start filebeat
root@elk:/etc/filebeat# systemctl status filebeat
● filebeat.service - Filebeat sends log files to Logstash or directly to Elasticsearch.
     Loaded: loaded (/usr/lib/systemd/system/filebeat.service; enabled; preset: enabled)
     Active: active (running) since Tue 2025-07-29 02:39:48 UTC; 6s ago
       Docs: https://www.elastic.co/beats/filebeat
   Main PID: 76722 (filebeat)
      Tasks: 8 (limit: 4549)
     Memory: 87.4M (peak: 87.7M)
        CPU: 1.276s
     CGroup: /system.slice/filebeat.service
             └─76722 /usr/share/filebeat/bin/filebeat --environment systemd -c /etc/filebeat/filebeat.yml --path.home /usr/share/filebeat --path.config /etc/filebeat --path.data /var/lib/filebeat --path.logs /var/log/filebeat

Jul 29 02:39:49 elk filebeat[76722]: {"log.level":"info","@timestamp":"2025-07-29T02:39:49.440Z","log.logger":"crawler","log.origin":{"function":"github.com/elastic/beats/v7/filebeat/beater.(*crawler).startInput","file.name":"beater/crawler.go","file.line":117},"message":"starting input, keys present on the config: [filebeat.inputs.0.enabled filebeat.inputs.0.id filebeat.inputs.0.paths.0 filebeat.inputs.0.type]","service.name":"filebeat","ecs.version":"1.6.0"}
Jul 29 02:39:49 elk filebeat[76722]: {"log.level":"info","@timestamp":"2025-07-29T02:39:49.441Z","log.logger":"crawler","log.origin":{"function":"github.com/elastic/beats/v7/filebeat/beater.(*crawler).startInput","file.name":"beater/crawler.go","file.line":148},"message":"Starting input (ID: 11337388005444501392)","service.name":"filebeat","ecs.version":"1.6.0"}
Jul 29 02:39:49 elk filebeat[76722]: {"log.level":"info","@timestamp":"2025-07-29T02:39:49.441Z","log.logger":"crawler","log.origin":{"function":"github.com/elastic/beats/v7/filebeat/beater.(*crawler).Start","file.name":"beater/crawler.go","file.line":106},"message":"Loading and starting Inputs completed. Enabled inputs: 1","service.name":"filebeat","ecs.version":"1.6.0"}
Jul 29 02:39:49 elk filebeat[76722]: {"log.level":"info","@timestamp":"2025-07-29T02:39:49.441Z","log.origin":{"function":"github.com/elastic/beats/v7/libbeat/cfgfile.(*Reloader).Run","file.name":"cfgfile/reload.go","file.line":163},"message":"Config reloader started","service.name":"filebeat","ecs.version":"1.6.0"}
Jul 29 02:39:49 elk filebeat[76722]: {"log.level":"info","@timestamp":"2025-07-29T02:39:49.441Z","log.origin":{"function":"github.com/elastic/beats/v7/libbeat/cfgfile.(*Reloader).Run","file.name":"cfgfile/reload.go","file.line":223},"message":"Loading of config files completed.","service.name":"filebeat","ecs.version":"1.6.0"}
Jul 29 02:39:49 elk filebeat[76722]: {"log.level":"info","@timestamp":"2025-07-29T02:39:49.441Z","log.logger":"input.filestream","log.origin":{"function":"github.com/elastic/beats/v7/filebeat/input/v2/compat.(*runner).Start.func1","file.name":"compat/compat.go","file.line":135},"message":"Input 'filestream' starting","service.name":"filebeat","id":"my-filestream-id","ecs.version":"1.6.0"}
Jul 29 02:39:49 elk filebeat[76722]: {"log.level":"info","@timestamp":"2025-07-29T02:39:49.441Z","log.logger":"metric_registry","log.origin":{"function":"github.com/elastic/beats/v7/libbeat/monitoring/inputmon.NewInputRegistry","file.name":"inputmon/input.go","file.line":63},"message":"registering","service.name":"filebeat","input_type":"filestream","id":"my-filestream-id","key":"my-filestream-id","uuid":"9b41d03b-3ff4-4c25-ac1d-5606ebabfbae","ecs.version":"1.6.0"}
Jul 29 02:39:52 elk filebeat[76722]: {"log.level":"info","@timestamp":"2025-07-29T02:39:52.430Z","log.logger":"add_cloud_metadata","log.origin":{"function":"github.com/elastic/beats/v7/libbeat/processors/add_cloud_metadata.(*addCloudMetadata).init.func1","file.name":"add_cloud_metadata/add_cloud_metadata.go","file.line":100},"message":"add_cloud_metadata: hosting provider type not detected.","service.name":"filebeat","ecs.version":"1.6.0"}
Jul 29 02:39:52 elk filebeat[76722]: {"log.level":"info","@timestamp":"2025-07-29T02:39:52.458Z","log.logger":"publisher_pipeline_output","log.origin":{"function":"github.com/elastic/beats/v7/libbeat/publisher/pipeline.(*netClientWorker).run","file.name":"pipeline/client_worker.go","file.line":138},"message":"Connecting to backoff(async(tcp://10.211.55.31:5044))","service.name":"filebeat","ecs.version":"1.6.0"}
Jul 29 02:39:52 elk filebeat[76722]: {"log.level":"info","@timestamp":"2025-07-29T02:39:52.461Z","log.logger":"publisher_pipeline_output","log.origin":{"function":"github.com/elastic/beats/v7/libbeat/publisher/pipeline.(*netClientWorker).run","file.name":"pipeline/client_worker.go","file.line":146},"message":"Connection to backoff(async(tcp://10.211.55.31:5044)) established","service.name":"filebeat","ecs.version":"1.6.0"}
root@elk:/etc/filebeat# systemctl enable filebeat
Synchronizing state of filebeat.service with SysV service script with /usr/lib/systemd/systemd-sysv-install.
Executing: /usr/lib/systemd/systemd-sysv-install enable filebeat
root@elk:/etc/filebeat#

### we will see on KIBANA if the logs locadtion that is defined in the filebeat.yml are ingested or not on node-1 which is our master/elasticsearch/kibana
http://10.211.55.11:5601
### I couldnt find the logs in kibana, so we will take a look at filebeat logs 
root@elk:/etc/filebeat# systemctl status filebeat
● filebeat.service - Filebeat sends log files to Logstash or directly to Elasticsearch.
...
----------------> ### log location    └─76722 /usr/share/filebeat/bin/filebeat --environment systemd -c /etc/filebeat/filebeat.yml --path.home /usr/share/filebeat --path.config /etc/filebeat --path.data /var/lib/filebeat --path.logs /var/log/filebeat
...

### not being able to find the logs for the filebeat, stopping the filebeat for a while.
root@elk:/etc/filebeat# systemctl stop filebeat
root@elk:/etc/filebeat#
### looking in the 
root@elk:/etc/filebeat# systemctl start filebeat
root@elk:/etc/filebeat# systemctl status filebeat
● filebeat.service - Filebeat sends log files to Logstash or directly to Elasticsearch.
     Loaded: loaded (/usr/lib/systemd/system/filebeat.service; enabled; preset: enabled)	<-------------------------------------------
     Active: active (running) since Tue 2025-07-29 03:08:48 UTC; 4s ago
       Docs: https://www.elastic.co/beats/filebeat
   Main PID: 76898 (filebeat)
      Tasks: 7 (limit: 4549)
     Memory: 60.9M (peak: 61.1M)
        CPU: 169ms
     CGroup: /system.slice/filebeat.service
             └─76898 /usr/share/filebeat/bin/filebeat --environment systemd -c /etc/filebeat/filebeat.yml --path.home /usr/share/filebeat --path.config /etc/filebeat --path.data /var/lib/filebeat --path.logs /var/log/filebeat

root@elk:/etc/filebeat# vim /usr/lib/systemd/system/filebeat.service

### filebeat isnt writing the logs, because the logs location which we are trying to find is correct (/var/log/filebeat)
"/usr/lib/systemd/system/filebeat.service" 18L, 662B                                                                                                                                                                                             1,1           All
[Unit]
Description=Filebeat sends log files to Logstash or directly to Elasticsearch.
Documentation=https://www.elastic.co/beats/filebeat
Wants=network-online.target
After=network-online.target

[Service]

UMask=0027
Environment="GODEBUG='madvdontneed=1'"
Environment="BEAT_LOG_OPTS="
Environment="BEAT_CONFIG_OPTS=-c /etc/filebeat/filebeat.yml"
Environment="BEAT_PATH_OPTS=--path.home /usr/share/filebeat --path.config /etc/filebeat --path.data /var/lib/filebeat --path.logs /var/log/filebeat"
ExecStart=/usr/share/filebeat/bin/filebeat --environment systemd $BEAT_LOG_OPTS $BEAT_CONFIG_OPTS $BEAT_PATH_OPTS
Restart=always

[Install]
WantedBy=multi-user.target

### manually creating the filebeat log file in /var/log 
root@elk:/var/log# mkdir filebeat
root@elk:/var/log# cd /var/log
root@elk:/var/log# ls
README  alternatives.log  apport.log  apt  bootstrap.log  btmp  cloud-init-output.log  cloud-init.log  dist-upgrade  dpkg.log  elasticsearch  faillog  filebeat  fontconfig.log  installer  journal  lastlog  private  unattended-upgrades  wtmp
root@elk:/var/log# mkdir filebeat
mkdir: cannot create directory ‘filebeat’: File exists
root@elk:/var/log# cd filebeat/
root@elk:/var/log/filebeat#

### The instructor is mentioning that, he needs to create the inbound rule for the logstash port and that's how filebeat will be able to connect to the logstash server
### HYPOTHESIS: I need to create similar firewall rule on the ubuntu server probably so that telnet is able to listen
### telnetting from the Node-6 filebeat server to Node-5 logstash server port
root@elk:/var/log/filebeat# telnet 10.211.55.31 5044
Trying 10.211.55.31...
Connected to 10.211.55.31.	<------------------------------------------ ### listening to the server port, meaning the server port 5044 is open and listening since it is not rejected
Escape character is '^]'.
### I didnt have to do anything, seems it is listening 

### stop the manually running logstash on node-5
### add the debug line in the logstash conf.d/filebeat.conf file
### we are going to add another output in the logstash conf file file to print the log and then index it to elasticsearch
root@elk:/etc/logstash/conf.d# vim filebeat.conf
"filebeat.conf" 22L, 296B                                                                                                                                                                                                                                              14,20         All
input {
  beats {
    port => 5044

  }
}

filter {

}

output
{

stdout {
  codec => rubydebug	<--------------------------- ### added this line
}

  elasticsearch {
    hosts => ["https://sys.fcc:9200"]
    index => "logs_for_test_%{+YYYY.MM.dd}"
    user => elastic
    password => blfOwgJUMUuLKjmXVYU2
    ssl => true
    cacert => "/etc/elasticsearch/certs/ca/ca.crt"
  }
}
:wq!

### whatever the activities that is going to happen with the pipeline will be printed on the console as well.
### start the logstash again
root@elk:/etc/logstash/conf.d# /usr/share/logstash/bin/logstash -f /etc/logstash/conf.d/filebeat.conf
Using bundled JDK: /usr/share/logstash/jdk
OpenJDK 64-Bit Server VM warning: Unable to get SVE vector length on this system. Disabling SVE. Specify -XX:UseSVE=0 to shun this warning.
OpenJDK 64-Bit Server VM warning: Unable to get SVE vector length on this system. Disabling SVE. Specify -XX:UseSVE=0 to shun this warning.
WARNING: Could not find logstash.yml which is typically located in $LS_HOME/config or /etc/logstash. You can specify the path using --path.settings. Continuing using the defaults
Could not find log4j2 configuration at path /usr/share/logstash/config/log4j2.properties. Using default config which logs errors to the console
[WARN ] 2025-07-29 03:35:43.345 [main] runner - Starting from version 9.0, running with superuser privileges is not permitted unless you explicitly set 'allow_superuser' to true, thereby acknowledging the possible security risks
[WARN ] 2025-07-29 03:35:43.346 [main] runner - NOTICE: Running Logstash as a superuser is strongly discouraged as it poses a security risk. Set 'allow_superuser' to false for better security.
[WARN ] 2025-07-29 03:35:43.348 [main] runner - 'pipeline.buffer.type' setting is not explicitly defined.Before moving to 9.x set it to 'heap' and tune heap size upward, or set it to 'direct' to maintain existing behavior.
[INFO ] 2025-07-29 03:35:43.349 [main] runner - Starting Logstash {"logstash.version"=>"8.18.4", "jruby.version"=>"jruby 9.4.9.0 (3.1.4) 2024-11-04 547c6b150e OpenJDK 64-Bit Server VM 21.0.7+6-LTS on 21.0.7+6-LTS +indy +jit [aarch64-linux]"}
[INFO ] 2025-07-29 03:35:43.350 [main] runner - JVM bootstrap flags: [-Xms1g, -Xmx1g, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djruby.compile.invokedynamic=true, -XX:+HeapDumpOnOutOfMemoryError, -Djava.security.egd=file:/dev/urandom, -Dlog4j2.isThreadContextMapInheritable=true, -Djruby.regexp.interruptible=true, -Djdk.io.File.enableADS=true, --add-exports=jdk.compiler/com.sun.tools.javac.api=ALL-UNNAMED, --add-exports=jdk.compiler/com.sun.tools.javac.file=ALL-UNNAMED, --add-exports=jdk.compiler/com.sun.tools.javac.parser=ALL-UNNAMED, --add-exports=jdk.compiler/com.sun.tools.javac.tree=ALL-UNNAMED, --add-exports=jdk.compiler/com.sun.tools.javac.util=ALL-UNNAMED, --add-opens=java.base/java.security=ALL-UNNAMED, --add-opens=java.base/java.io=ALL-UNNAMED, --add-opens=java.base/java.nio.channels=ALL-UNNAMED, --add-opens=java.base/sun.nio.ch=ALL-UNNAMED, --add-opens=java.management/sun.management=ALL-UNNAMED, -Dio.netty.allocator.maxOrder=11]
[INFO ] 2025-07-29 03:35:43.401 [main] StreamReadConstraintsUtil - Jackson default value override `logstash.jackson.stream-read-constraints.max-string-length` configured to `200000000` (logstash default)
[INFO ] 2025-07-29 03:35:43.401 [main] StreamReadConstraintsUtil - Jackson default value override `logstash.jackson.stream-read-constraints.max-number-length` configured to `10000` (logstash default)
[INFO ] 2025-07-29 03:35:43.401 [main] StreamReadConstraintsUtil - Jackson default value override `logstash.jackson.stream-read-constraints.max-nesting-depth` configured to `1000` (logstash default)
[WARN ] 2025-07-29 03:35:43.479 [LogStash::Runner] multilocal - Ignoring the 'pipelines.yml' file because modules or command line options are specified
[INFO ] 2025-07-29 03:35:43.630 [Api Webserver] agent - Successfully started Logstash API endpoint {:port=>9600, :ssl_enabled=>false}
[INFO ] 2025-07-29 03:35:43.732 [Converge PipelineAction::Create<main>] Reflections - Reflections took 50 ms to scan 1 urls, producing 150 keys and 529 values
[WARN ] 2025-07-29 03:35:43.874 [Converge PipelineAction::Create<main>] elasticsearch - You are using a deprecated config setting "cacert" set in elasticsearch. Deprecated settings will continue to work, but are scheduled for removal from logstash in the future. Set 'ssl_certificate_authorities' instead. If you have any questions about this, please ask it on the https://discuss.elastic.co/c/logstash discussion forum {:name=>"cacert", :plugin=><LogStash::Outputs::ElasticSearch password=><password>, hosts=>[https://sys.fcc:9200], cacert=>"/etc/elasticsearch/certs/ca/ca.crt", index=>"logs_for_test_%{+YYYY.MM.dd}", id=>"1f45a3c3742417904a17ddcebf8b705d6bf69806b930c3cb6e48857b96105a9f", user=>"elastic", ssl=>true, enable_metric=>true, codec=><LogStash::Codecs::Plain id=>"plain_3a834685-a583-4d5a-ad9b-f494ca5e7c13", enable_metric=>true, charset=>"UTF-8">, workers=>1, ssl_certificate_verification=>true, ssl_verification_mode=>"full", sniffing=>false, sniffing_delay=>5, timeout=>60, pool_max=>1000, pool_max_per_route=>100, resurrect_delay=>5, validate_after_inactivity=>10000, http_compression=>true, compression_level=>1, retry_initial_interval=>2, retry_max_interval=>64, dlq_on_failed_indexname_interpolation=>true, data_stream_type=>"logs", data_stream_dataset=>"generic", data_stream_namespace=>"default", data_stream_sync_fields=>true, data_stream_auto_routing=>true, manage_template=>true, template_overwrite=>false, template_api=>"auto", doc_as_upsert=>false, script_type=>"inline", script_lang=>"painless", script_var_name=>"event", scripted_upsert=>false, retry_on_conflict=>1, ilm_enabled=>"auto", ilm_pattern=>"{now/d}-000001", ilm_policy=>"logstash-policy">}
[WARN ] 2025-07-29 03:35:43.877 [Converge PipelineAction::Create<main>] elasticsearch - You are using a deprecated config setting "cacert" set in elasticsearch. Deprecated settings will continue to work, but are scheduled for removal from logstash in the future. Set 'ssl_certificate_authorities' instead. If you have any questions about this, please ask it on the https://discuss.elastic.co/c/logstash discussion forum
[WARN ] 2025-07-29 03:35:43.877 [Converge PipelineAction::Create<main>] elasticsearch - You are using a deprecated config setting "ssl" set in elasticsearch. Deprecated settings will continue to work, but are scheduled for removal from logstash in the future. Set 'ssl_enabled' instead. If you have any questions about this, please ask it on the https://discuss.elastic.co/c/logstash discussion forum {:name=>"ssl", :plugin=><LogStash::Outputs::ElasticSearch password=><password>, hosts=>[https://sys.fcc:9200], cacert=>"/etc/elasticsearch/certs/ca/ca.crt", index=>"logs_for_test_%{+YYYY.MM.dd}", id=>"1f45a3c3742417904a17ddcebf8b705d6bf69806b930c3cb6e48857b96105a9f", user=>"elastic", ssl=>true, enable_metric=>true, codec=><LogStash::Codecs::Plain id=>"plain_3a834685-a583-4d5a-ad9b-f494ca5e7c13", enable_metric=>true, charset=>"UTF-8">, workers=>1, ssl_certificate_verification=>true, ssl_verification_mode=>"full", sniffing=>false, sniffing_delay=>5, timeout=>60, pool_max=>1000, pool_max_per_route=>100, resurrect_delay=>5, validate_after_inactivity=>10000, http_compression=>true, compression_level=>1, retry_initial_interval=>2, retry_max_interval=>64, dlq_on_failed_indexname_interpolation=>true, data_stream_type=>"logs", data_stream_dataset=>"generic", data_stream_namespace=>"default", data_stream_sync_fields=>true, data_stream_auto_routing=>true, manage_template=>true, template_overwrite=>false, template_api=>"auto", doc_as_upsert=>false, script_type=>"inline", script_lang=>"painless", script_var_name=>"event", scripted_upsert=>false, retry_on_conflict=>1, ilm_enabled=>"auto", ilm_pattern=>"{now/d}-000001", ilm_policy=>"logstash-policy">}
[WARN ] 2025-07-29 03:35:43.877 [Converge PipelineAction::Create<main>] elasticsearch - You are using a deprecated config setting "ssl" set in elasticsearch. Deprecated settings will continue to work, but are scheduled for removal from logstash in the future. Set 'ssl_enabled' instead. If you have any questions about this, please ask it on the https://discuss.elastic.co/c/logstash discussion forum
[INFO ] 2025-07-29 03:35:43.885 [Converge PipelineAction::Create<main>] javapipeline - Pipeline `main` is configured with `pipeline.ecs_compatibility: v8` setting. All plugins in this pipeline will default to `ecs_compatibility => v8` unless explicitly configured otherwise.
[INFO ] 2025-07-29 03:35:43.891 [[main]-pipeline-manager] elasticsearch - New Elasticsearch output {:class=>"LogStash::Outputs::ElasticSearch", :hosts=>["https://sys.fcc:9200"]}
[INFO ] 2025-07-29 03:35:43.933 [[main]-pipeline-manager] elasticsearch - Elasticsearch pool URLs updated {:changes=>{:removed=>[], :added=>[https://elastic:xxxxxx@sys.fcc:9200/]}}
[WARN ] 2025-07-29 03:35:44.046 [[main]-pipeline-manager] elasticsearch - Restored connection to ES instance {:url=>"https://elastic:xxxxxx@sys.fcc:9200/"}
[INFO ] 2025-07-29 03:35:44.047 [[main]-pipeline-manager] elasticsearch - Elasticsearch version determined (8.18.1) {:es_version=>8}
[INFO ] 2025-07-29 03:35:44.052 [[main]-pipeline-manager] elasticsearch - Not eligible for data streams because config contains one or more settings that are not compatible with data streams: {"index"=>"logs_for_test_%{+YYYY.MM.dd}"}
[INFO ] 2025-07-29 03:35:44.052 [[main]-pipeline-manager] elasticsearch - Data streams auto configuration (`data_stream => auto` or unset) resolved to `false`
[INFO ] 2025-07-29 03:35:44.060 [[main]-pipeline-manager] javapipeline - Starting pipeline {:pipeline_id=>"main", "pipeline.workers"=>2, "pipeline.batch.size"=>125, "pipeline.batch.delay"=>50, "pipeline.max_inflight"=>250, "pipeline.sources"=>["/etc/logstash/conf.d/filebeat.conf"], :thread=>"#<Thread:0x2f04ffa7 /usr/share/logstash/logstash-core/lib/logstash/java_pipeline.rb:138 run>"}
[INFO ] 2025-07-29 03:35:44.080 [Ruby-0-Thread-10: /usr/share/logstash/vendor/bundle/jruby/3.1.0/gems/logstash-output-elasticsearch-11.22.12-java/lib/logstash/plugin_mixins/elasticsearch/common.rb:164] elasticsearch - Using a default mapping template {:es_version=>8, :ecs_compatibility=>:v8}
[INFO ] 2025-07-29 03:35:44.266 [[main]-pipeline-manager] javapipeline - Pipeline Java execution initialization time {"seconds"=>0.21}
[INFO ] 2025-07-29 03:35:44.269 [[main]-pipeline-manager] beats - Starting input listener {:address=>"0.0.0.0:5044"}
[INFO ] 2025-07-29 03:35:44.273 [[main]-pipeline-manager] javapipeline - Pipeline started {"pipeline.id"=>"main"}
[INFO ] 2025-07-29 03:35:44.282 [Agent thread] agent - Pipelines running {:count=>1, :running_pipelines=>[:main], :non_running_pipelines=>[]}
[INFO ] 2025-07-29 03:35:44.301 [[main]<beats] Server - Starting server on port: 5044

INDEX SHOWED UP IN THE KIBANA
click on hamburger drop down menu > stack management > index management > indices
logs_for_test_2025.07.29	green	open	1	1	9,595	5.02mb


RESOLUTION:


SIDENOTE: 
iis.conf iis is internet information service which is used in every organization. Either you host your website apache, tomcat or nginx webhosting system but for the windows it gets hosted over the iis. we need to monitor the IIS logs to see how much traffic is there, what activity is happening with webapplication. All that these gets monitored by the beats and then gets parsing. This happens through the logstash conf.d configuration file e.g iis.conf

do homework on 
- SHA256 vs RSA and how two different are used and what is difference between fingerprint of data and encryption
- dissect vs GROK
	dissect -- matched fields
	GROK -- regex




CONT'...

filebeat.yml 
input >> /file
output >> logstash:5055

Logstash/conf.d/<file.conf>
input >> beats 5044
filter >>
output >> ELK

Agents:
in ELK stack these are all the agents:
Beats (file, metric, heart)
logstash
other agents

Ui: 
Kibana

clear
Database: 
ELK

### command to run the logstash
root@elk:/etc/logstash/conf.d# /usr/share/logstash/bin/logstash -f /etc/logstash/conf.d/filebeat.conf

### on filebeat server node 6 run filebeat
root@elk:/var/log# systemctl start filebeat
root@elk:/var/log# vim my_data.log
testing
wq!

### OUTPUT ON NODE 5 logstash 
{
         "agent" => {
                "name" => "elk",
                "type" => "filebeat",
        "ephemeral_id" => "0b2f8e70-db95-437d-a46e-fc9b24b5846b",
                  "id" => "d7a4dfcb-de4e-4bbf-b0a1-060c65dab621",
             "version" => "8.18.1"
    },
         "event" => {
        "original" => "2025-08-03 01:37:41 status installed telnet:all 0.17+2.5-3ubuntu4"
    },
          "host" => {
                  "mac" => [
            [0] "00-1C-42-6D-E0-89"
        ],
        "containerized" => false,
             "hostname" => "elk",
                   "os" => {
              "kernel" => "6.8.0-64-generic",
                "name" => "Ubuntu",
            "codename" => "noble",
                "type" => "linux",
            "platform" => "ubuntu",
              "family" => "debian",
             "version" => "24.04.2 LTS (Noble Numbat)"
        },
                   "ip" => [
            [0] "10.211.55.32",
            [1] "fdb2:2c26:f4e4:0:21c:42ff:fe6d:e089",
            [2] "fe80::21c:42ff:fe6d:e089"
        ],
                 "name" => "elk",
         "architecture" => "aarch64",
                   "id" => "53d351d845dc4ea09111e51cc53b4b62"
    },
          "tags" => [
        [0] "beats_input_codec_plain_applied"
    ],
           "ecs" => {
        "version" => "8.0.0"
    },
           "log" => {
        "offset" => 578296,
          "file" => {
                "inode" => "133150",
                 "path" => "/var/log/dpkg.log",
            "device_id" => "64512"
        }
    },
       "message" => "2025-08-03 01:37:41 status installed telnet:all 0.17+2.5-3ubuntu4",
      "@version" => "1",
    "@timestamp" => 2025-08-03T01:37:41.899Z,
         "input" => {
        "type" => "filestream"
    }
}
{
         "agent" => {
                "name" => "elk",
                "type" => "filebeat",
        "ephemeral_id" => "0b2f8e70-db95-437d-a46e-fc9b24b5846b",
                  "id" => "d7a4dfcb-de4e-4bbf-b0a1-060c65dab621",
             "version" => "8.18.1"
    },
         "event" => {
        "original" => "testing"
    },
          "host" => {
                  "mac" => [
            [0] "00-1C-42-6D-E0-89"
        ],
        "containerized" => false,
             "hostname" => "elk",
                   "os" => {
              "kernel" => "6.8.0-64-generic",
                "name" => "Ubuntu",
            "codename" => "noble",
                "type" => "linux",
            "platform" => "ubuntu",
              "family" => "debian",
             "version" => "24.04.2 LTS (Noble Numbat)"
        },
                 "name" => "elk",
                   "ip" => [
            [0] "10.211.55.32",
            [1] "fdb2:2c26:f4e4:0:21c:42ff:fe6d:e089",
            [2] "fe80::21c:42ff:fe6d:e089"
        ],
         "architecture" => "aarch64",
                   "id" => "53d351d845dc4ea09111e51cc53b4b62"
    },
          "tags" => [
        [0] "beats_input_codec_plain_applied"
    ],
           "ecs" => {
        "version" => "8.0.0"
    },
           "log" => {
        "offset" => 0,
          "file" => {
                "inode" => "131640",
                 "path" => "/var/log/my_data.log",
            "device_id" => "64512"
        }
    },
       "message" => "testing",
      "@version" => "1",
    "@timestamp" => 2025-08-03T03:08:45.890Z,
         "input" => {
        "type" => "filestream"
    }
}



### on Kibana 
next go to the kibana, go to stack management and you will see logs_for_test_<alias rolloever date>
next create the data view 
Dataview > index pattern
logs_for_test*


### parsening and FILTERING LOGS



### GROK
### next we will parse the logs to create more fields and apply the grok pattern in the message field and create more fields out of it. 
