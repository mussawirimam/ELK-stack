### flow:
ingest pipeline > put ingest pipeline > simulate the ingest pipeline with document > specify ingest pipeline in and specify log path in filebeat.yml > start filebeat
if its not indexing delete the stream from stack management > stop filebeat service and restart 

### what is ingest pipeline: ingest pipeline is a json configuration where you define the name of the ingest pipeline, descripton, processors, and patthern if a processor requires. you can get more information how error handling will be used in code through
official documentation of elasticsearch. 

### simulating pipeline is the most important step, use the below json configuration: 
POST _ingest/pipeline/apache_logs_pipeline/_simulate
{
  "docs": [
    {
      "_source": {
        "message": "192.168.1.100 - - [27/Mar/2025:12:45:32 +0000] \"GET /index.html HTTP/1.1\" 200 5324"
      }
    }
  ]
}

### to get the ingest pipeline thats been put
GET _ingest/pipeline/my_pipeline

### searching for the default filebeat index which is created by the filebeat agent itself it not other index is specified in filebeat.yml configuration.  
GET filebeat-*/_search 


--------------------------------------------------------------
### TARGET LOG that exists within the apache.log on server
127.0.0.1 - dave [11/Jan/2026:12:09:33 +0000] "GET /api/data HTTP/1.1" 500 231 "-" "PostmanRuntime/7.36.1"

### TASK: need to create unstructured logs into structured logs and breakdown information in message field into new fields
### this ingest pipeline logs the unstructured apache logs to structured logs and add 

### PIPELINE: 
PUT _ingest/pipeline/apache_access_pipeline
{
  "description": "Parse Apache access logs safely (no dropping)",
  "processors": [
    {
      "grok": {
        "field": "message",
        "patterns": [
          "%{IP:client.ip} - %{DATA:user.name} \\[%{HTTPDATE:apache.timestamp}\\] \"%{WORD:http.request.method} %{URIPATHPARAM:url.path} HTTP/%{NUMBER:http.version}\" %{NUMBER:http.response.status_code} %{NUMBER:http.response.body.bytes} \"%{DATA:http.request.referrer}\" \"%{DATA:user_agent.original}\""
        ],
        "ignore_failure": true
      }
    },
    {
      "date": {
        "field": "apache.timestamp",
        "formats": ["dd/MMM/yyyy:HH:mm:ss Z"],
        "target_field": "@timestamp",
        "ignore_failure": true
      }
    },
    {
      "remove": {
        "field": "apache.timestamp",
        "ignore_failure": true
      }
    }
  ]
}

### OUTPUT FROM THE SIMULATION
{
  "docs": [
    {
      "doc": {
        "_index": ".ds-filebeat-8.17.0-2026.01.16-000001",
        "_version": "-3",
        "_id": "sgwpxZsBoLzpx93GxXbZ",
        "_source": {
          "agent": {
            "name": "elk",
            "id": "6c0ce29e-6873-4137-9901-66b87b5e2b86",
            "type": "filebeat",
            "ephemeral_id": "71bf0658-959b-4f28-9b4d-a299775e6d19",
            "version": "8.17.0"
          },
          "log": {
            "offset": 2595,
            "file": {
              "path": "/home/elk/filebeat/apache.log"
            }
          },
          "message": "127.0.0.1 - dave [11/Jan/2026:12:09:33 +0000] \"GET /api/data HTTP/1.1\" 500 231 \"-\" \"PostmanRuntime/7.36.1\"",
          "url": {
            "path": "/api/data"
          },
          "tags": [
            "apache"
          ],
          "input": {
            "type": "log"
          },
          "@timestamp": "2026-01-11T12:09:33.000Z",
          "apache": {},
          "ecs": {
            "version": "8.0.0"
          },
          "host": {
            "name": "elk"
          },
          "http": {
            "request": {
              "method": "GET",
              "referrer": "-"
            },
            "version": "1.1",
            "response": {
              "body": {
                "bytes": "231"
              },
              "status_code": "500"
            }
          },
          "client": {
            "ip": "127.0.0.1"
          },
          "user": {
            "name": "dave"
          },
          "user_agent": {
            "original": "PostmanRuntime/7.36.1"
          }
        },
        "_ingest": {
          "timestamp": "2026-01-16T05:28:21.139139127Z"
        }
      }
    }
  ]
}
### OUTPUT FROM THE SIMULATION ENDS
--------------------------------------------------------------
--------------------------------------------------------------
### TARGET LOG: is json structured logs
{
  "targets": [
    "localhost:9100"
  ],
  "labels": {
    "application": "linux",
    "host": "local",
    "region": "wsl"
  }
}
### I am adding it to single line for the ingest pipeline for the ease sake. Its to above 
{"targets":["localhost:9100"],"labels":{"application":"linux","host":"local","region":"wsl"}}

### TASK: we need to break down breakdown the key-value pair in new fields from message field
### json format pipeline. 

### PIPELINE: 
PUT _ingest/pipeline/json_logs_pipeline
{
  "description": "Pipeline to parse JSON logs",
  "processors": [
    {
      "json": {
        "field": "message",
        "target_field": "log_data"
      }
    }
]}

### not sure how they are doing it in prod env but in my local machine im specifying the name of the ingest pipeline in the filebeat.yml and the log path. 


### OUTPUT FROM THE SIMULATION
{
  "_index": ".ds-filebeat-8.17.0-2026.01.17-000001",
  "_id": "xgzizZsBoLzpx93Gf3bo",
  "_version": 1,
  "_score": 0,
  "_source": {
    "input": {
      "type": "log"
    },
    "agent": {
      "name": "elk",
      "id": "6c0ce29e-6873-4137-9901-66b87b5e2b86",
      "ephemeral_id": "9379ee3d-6e19-4dc7-8d0c-efdc0b0d3c4c",
      "type": "filebeat",
      "version": "8.17.0"
    },
    "@timestamp": "2026-01-17T21:35:21.645Z",
    "ecs": {
      "version": "8.0.0"
    },
    "log": {
      "file": {
        "path": "/home/elk/filebeat/json.log"
      },
      "offset": 470
    },
    "host": {
      "name": "elk"
    },
    "log_data": {
      "targets": [
        "localhost:9100"
      ],
      "labels": {
        "application": "linux",
        "host": "local",
        "region": "wsl"
      }
    },
    "message": "{\"targets\":[\"localhost:9100\"],\"labels\":{\"application\":\"linux\",\"host\":\"local\",\"region\":\"wsl\"}}",
    "tags": [
      "apache"
    ]
  },
  "fields": {
    "log_data.labels.application": [
      "linux"
    ],
    "input.type": [
      "log"
    ],
    "log.offset": [
      470
    ],
    "agent.hostname": [
      "elk"
    ],
    "message": [
      "{\"targets\":[\"localhost:9100\"],\"labels\":{\"application\":\"linux\",\"host\":\"local\",\"region\":\"wsl\"}}"
    ],
    "log_data.labels.host": [
      "local"
    ],
    "tags": [
      "apache"
    ],
    "agent.type": [
      "filebeat"
    ],
    "@timestamp": [
      "2026-01-17T21:35:21.645Z"
    ],
    "agent.id": [
      "6c0ce29e-6873-4137-9901-66b87b5e2b86"
    ],
    "ecs.version": [
      "8.0.0"
    ],
    "log_data.targets": [
      "localhost:9100"
    ],
    "log.file.path": [
      "/home/elk/filebeat/json.log"
    ],
    "log_data.labels.region": [
      "wsl"
    ],
    "agent.ephemeral_id": [
      "9379ee3d-6e19-4dc7-8d0c-efdc0b0d3c4c"
    ],
    "agent.name": [
      "elk"
    ],
    "agent.version": [
      "8.17.0"
    ],
    "host.name": [
      "elk"
    ]
  }
}




BONUS:
GET filebeat-*/_count
GET filebeat-*/_search
GET filebeat-*/_settings
GET filebeat-*/_settings?filter_path=**.default_pipeline,**.final_pipeline
GET filebeat-*/_settings?filter_path=**pipeline*
GET filebeat-*/_doc
GET filebeat-*/_search
{
  "query": {
    "ids": {
      "values": ["hwxrrpsBoLzpx93GxXZc"]
    }
  }
}

### to test if the count has increased or decreased for the documents getting parsed into elasticsearch.
GET filebeat-*/_count

NOTE:
### for testing its ok to delete the stream/index and reindex/populate it back again through stopping filebeat and starting it
### but in prod you should never do that.

### gives output of actual documents within the filebeat-* dataview
GET filebeat-*/_search

--------------------------------------------------------------

### RUFF: 
# Click the Variables button, above, to create your own variables.
GET ${exampleVariable1} // _search
{
  "query": {
    "${exampleVariable2}": {} // match_all
  }
}

### APACHE INGEST PIPELINE E.G
### these two pipelines are functionally similar, but they are not equivalent. They parse the same Apache log format, but they differ in field naming, structure, and downstream usefulness.
### this ingest pipeline logs the unstructured apache logs to structured logs and add 
### ignore the pipeline below
PUT _ingest/pipeline/apache_logs_pipeline
{
  "description": "Parse Apache access logs safely",
  "processors": [
    {
      "grok": {
        "field": "message",
        "patterns": [
          "%{IP:ip} - %{DATA:user} \\[%{HTTPDATE:timestamp}\\] \"%{WORD:method} %{URIPATHPARAM:uri} HTTP/%{NUMBER:http_version}\" %{NUMBER:response} %{NUMBER:bytes} \"%{DATA:referrer}\" \"%{DATA:user_agent}\""
        ],
        "ignore_failure": true
      }
    },
    {
      "date": {
        "field": "timestamp",
        "formats": ["dd/MMM/yyyy:HH:mm:ss Z"],
        "ignore_failure": true
      }
    }
  ]
}
