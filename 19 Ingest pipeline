27-3-2025
teacher re-caps the database to student and begins the ingest pipeline

intro 
what is the ingest pipeline? 
elasticsearch allows you to to preprocess documents before indexing them. They are particularly useful when working with raw log data that needs transformation, enrichement, or cleanup.

So earilier, we were working with the filebeat and logstash and you dont have the need for logstash. And you will install a filebeat 
in logstash you use the filter and in ingest pipeline you will use the processors

+------------------+
|   Application    |
|   / System Logs  |
|  (Server / VM)   |
+--------+---------+
         |
         | 1. Logs written to files
         v
+------------------+
|     Filebeat     |
|  (Lightweight    |
|   Shipper)       |
|                  |
| - Reads log files|
| - Adds metadata  |
| - Parses basics  |
+--------+---------+
         |
         | 2. Beats Protocol (HTTP/JSON)
         v
+------------------------------+
|        Elasticsearch         |
|   (Ingest Node Enabled)      |
|                              |
|  +------------------------+  |
|  |   Ingest Pipeline      |  |
|  |------------------------|  |
|  | - Grok processors      |  |
|  | - Date processors      |  |
|  | - GeoIP / Enrich       |  |
|  | - Rename / Drop fields |  |
|  +------------------------+  |
|                              |
|  -> Indexed into Data Stream |
|     / Index                  |
+---------------+--------------+
                |
                | 3. Search / Query
                v
+------------------------------+
|           Kibana             |
|                              |
| - Discover                   |
| - Dashboards                 |
| - Alerts                     |
+------------------------------+

Mid

### we are going to ingest the apache logs and for that we will be creating the grok pattern in order to feed it through the ingest pipeline:
### use this Tool: https://grokdebugger.com
# apache grok pattern:
%{IP:IP} - %{DATA:Name} \[%{HTTPDATE:timestamp}\] \"%{DATA:Request} %{DATA:Uri}HTTP/1.0\" %{NUMBER:Response} %{NUMBER:Pid}
# apache log
127.0.0.1 - frank [10/Oct/2024:13:55:36 -0700] "GET /apache_pb.gif HTTP/1.0" 200 2326
########################################################################################
or you can use combined apache log GROK which makes things easy
PUT _ingest/pipeline/apache_access_pipeline
{
  "description": "Parse Apache access logs",
  "processors": [
    {
      "grok": {
        "field": "message",
        "patterns": [
          "%{COMBINEDAPACHELOG}"
        ]
      }
    },
    {
      "date": {
        "field": "timestamp",
        "formats": ["dd/MMM/yyyy:HH:mm:ss Z"],
        "ignore_failure": true
      }
    }
  ]
}


POST _ingest/pipeline/apache_access_pipeline/_simulate
{
  "docs": [
    {
      "_source": {
        "message": "127.0.0.1 - frank [18/Mar/2025:12:00:01 +0000] \"GET /index.html HTTP/1.1\" 200 1234 \"-\" \"Mozilla/5.0\""
      }
    }
  ]
}
OUTPUT:
===============
{
  "docs": [
    {
      "doc": {
        "_index": "_index",
        "_id": "_id",
        "_version": "-3",
        "_source": {
          "request": "/index.html",
          "agent": "\"Mozilla/5.0\"",
          "auth": "frank",
          "ident": "-",
          "verb": "GET",
          "message": "127.0.0.1 - frank [18/Mar/2025:12:00:01 +0000] \"GET /index.html HTTP/1.1\" 200 1234 \"-\" \"Mozilla/5.0\"",
          "referrer": "\"-\"",
          "@timestamp": "2025-03-18T12:00:01.000Z",
          "response": "200",
          "bytes": "1234",
          "clientip": "127.0.0.1",
          "httpversion": "1.1",
          "timestamp": "18/Mar/2025:12:00:01 +0000"
        },
        "_ingest": {
          "timestamp": "2026-01-02T03:38:53.653741833Z"
        }
      }
    }
  ]
}

### so instead of simulating to make permanent changes to the pipeline.
you can either use the index pattern to point the logs to the ingest pipeline or through filebeat.yml file you can point the logs to ingest pipeline. 
2. Point Filebeat to the pipeline (THIS makes it permanent)
In filebeat.yml:
output.elasticsearch:
  hosts: ["https://sys.fcc:9200"]
  pipeline: apache_access_pipeline
Restart Filebeat:
systemctl restart filebeat
‚úî Every new event from Filebeat now always goes through this pipeline
‚úÖ OPTION 2: Attach Pipeline at Index Level (Index Template)
Use this if multiple shippers write to the same index.
1. Create an index template
PUT _index_template/apache_logs_template
{
  "index_patterns": ["apache-logs-*"],
  "template": {
    "settings": {
      "index.default_pipeline": "apache_access_pipeline"
    }
  }
}
‚úî Any index matching apache-logs-* automatically uses the pipeline
‚úÖ OPTION 3: Attach Pipeline Per Request (Manual / Rare)
Only for manual indexing or testing:
POST apache-logs-2025.03.18/_doc?pipeline=apache_access_pipeline
{
  "message": "..."
}
‚ùå Not suitable for Filebeat
üî• COMMON MISTAKE (Very Important)
‚ÄúI created the pipeline, but nothing changed‚Äù
That happens because:
Pipelines do nothing until attached
Simulation does not apply them
Old data is never reprocessed
Only new documents go through the pipeline.
Best Practice Summary
Scenario	Best Method
Filebeat ‚Üí ES	output.elasticsearch.pipeline
Multiple sources	Index template
Manual tests	Request parameter
Final Recommendation (For Your Setup)
You already use Filebeat ‚Üí Elasticsearch.
Do this:
Filebeat ‚Üí output.elasticsearch.pipeline
Then:
Restart Filebeat
Ingest new logs
Verify parsed fields in Kibana
If you want, I can:
Give you exact Filebeat config
Convert this to IIS pipeline
Show how to migrate off Logstash safely
Add failure handling / dead-letter indexing
Say what you want next.


Conc

### Below is the documentation and simple grok pattern to use it for ingestion: 


Introduction to Ingest Pipelines
Ingest pipelines in Elastic Search allow you to preprocess documents before indexing them. They are particularly useful when working with raw log data that needs transformation, enrichment, or cleanup.
Why Use Ingest Pipelines?
‚Ä¢	Data Transformation: Modify fields, extract values, and clean up log entries.
‚Ä¢	Log Enrichment: Add metadata like geo-location, user agent, or timestamp formatting.
‚Ä¢	Efficient Parsing: Preprocess logs before they are stored in Elastic Search, reducing query-time processing.

Components of an Ingest Pipeline
An ingest pipeline consists of a series of processors that manipulate data before it is indexed. Common processors include:
‚Ä¢	Grok Processor: Pattern matching for extracting structured data from unstructured logs.
‚Ä¢	Date Processor: Converts date strings into proper timestamps.
‚Ä¢	Geo IP Processor: Enriches logs with geographical information.
‚Ä¢	Rename Processor: Renames fields for better consistency.

Setting Up an Ingest Pipeline
Step 1: Create an Ingest Pipeline
You can define an ingest pipeline using the Elastic Search API.
PUT _ingest/pipeline/my_pipeline
{
  "description": "Parse and structure logs",
  "processors": [
    {
      "grok": {
        "field": "message",
        "patterns": ["%{timestamp} %{+timestamp} %{IP} %{Method} %{URI} user=%{user} %{port} %{admin} %{client_IP} %{browser} %{responce_code} %{uri} %{uri2} %{number}"]
      }
    },
    {
      "date": {
        "field": "timestamp",
        "formats": ["dd/MMM/yyyy:HH:mm:ss Z"]
      }
    }
  ]
}

Step 2: Configure Filebeat to Use the Pipeline
Modify your filebeat.yml configuration to send logs through the pipeline:
filebeat.inputs:
  - type: log
    enabled: true
    paths:
      - /var/log/apache2/access.log
    pipeline: "my_pipeline"

Step 3: Verify Data in Elastic Search
After Filebeat sends logs, verify the processed output:
GET filebeat-*/_search
{
  "query": {
    "match_all": {}
  }
}





POST _ingest/pipeline/apache_logs_pipeline/_simulate
{
  "docs": [
    {
      "_source": {
        "message": "192.168.1.100 - - [27/Mar/2025:12:45:32 +0000] \"GET /index.html HTTP/1.1\" 200 5324"
      }
    }
  ]
}



Scenario 3: Processing JSON Logs
Use Case: Extract fields from JSON logs stored as a string.
Sample Log Entry
{
  "message": "{\"user\": \"john_doe\", \"action\": \"login\", \"status\": \"success\"}"
}
Ingest Pipeline Definition
‚Ä¢	Use json processor to parse message into separate fields.
PUT _ingest/pipeline/json_logs_pipeline
{
  "description": "Pipeline to parse JSON logs",
  "processors": [
    {
      "json": {
        "field": "message",
        "target_field": "log_data"
      }
    },
    {
      "rename": {
        "field": "log_data.user",
        "target_field": "user"
      }
    },
    {
      "rename": {
        "field": "log_data.action",
        "target_field": "action"
      }
    },
    {
      "rename": {
        "field": "log_data.status",
        "target_field": "status"
      }
    },
    {
      "remove": {
        "field": ["message", "log_data"]
      }
    }
  ]
}
Testing the Pipeline
POST _ingest/pipeline/json_logs_pipeline/_simulate
{
  "docs": [
    {
      "_source": {
        "message": "{\"user\": \"john_doe\", \"action\": \"login\", \"status\": \"success\"}"
      }
    }
  ]
}
 
Scenario 4: Masking Sensitive Data
Use Case: Remove or mask sensitive information (e.g., credit card numbers).
Sample Log Entry
User: John Doe, Credit Card: 4111-1111-1111-1111, Action: Purchase
Ingest Pipeline Definition
‚Ä¢	Use gsub processor to mask credit card numbers.
PUT _ingest/pipeline/mask_sensitive_data_pipeline
{
  "description": "Pipeline to mask credit card details",
  "processors": [
    {
      "gsub": {
        "field": "message",
        "pattern": "\\b\\d{4}-\\d{4}-\\d{4}-\\d{4}\\b",
        "replacement": "****-****-****-****"
      }
    }
  ]
}
Testing the Pipeline
POST _ingest/pipeline/mask_sensitive_data_pipeline/_simulate
{
  "docs": [
    {
      "_source": {
        "message": "User: John Doe, Credit Card: 4111-1111-1111-1111, Action: Purchase"
      }
    }
  ]
}
 
Scenario 5: Automatically Categorizing Log Severity
Use Case: Add a log_level field based on keywords.
Sample Log Entry
 [ERROR] Database connection failed.
Ingest Pipeline Definition
‚Ä¢	Use set processor to assign a log_level based on message content.
PUT _ingest/pipeline/log_severity_pipeline
{
  "description": "Pipeline to categorize logs by severity",
  "processors": [
    {
      "grok": {
        "field": "message",
        "patterns": ["\\[%{WORD:log_level}\\] %{GREEDYDATA:log_message}"]
      }
    },
    {
      "lowercase": {
        "field": "log_level"
      }
    }
  ]
}
Testing the Pipeline
POST _ingest/pipeline/log_severity_pipeline/_simulate
{
  "docs": [
    {
      "_source": {
        "message": "[ERROR] Database connection failed."
      }
    }
  ]
}

Hands-On Exercise
1.	Install File beat and configure it to read sample logs.
2.	Create an ingest pipeline in Elastic Search.
3.	Modify Filebeat to send logs through the pipeline.
 
This guide provides a structured approach for students to learn about ingest pipelines in a practical way. You can modify the exercises to include IIS logs or other log sources based on your teaching needs.
1.	Check transformed data in Kibana.




Conclusion
Using ingest pipelines with File beat simplifies log processing and enhances data quality before indexing. This approach makes Elastic Search more efficient for searching and analyzing log data.

