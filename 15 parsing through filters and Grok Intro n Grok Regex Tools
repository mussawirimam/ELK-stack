IMPORTANT:
### GROK DOCUMENTATION:
https://www.elastic.co/docs/explore-analyze/scripting/grok


TOOLS:
https://regexr.com
https://grokdebugger.com
https://github.com/cjslack/grok-debugger/blob/master/public/patterns/grok-patterns

### now that we have learned how to setup the filebeat and logstash servers and their configuration and basics.
### we will learn how to parse the data through the filter first and learn about the GROK

basic of conf'd pipeline
input
filter  <------------------ ### we learned input and output, now we will learn about the parseing data/logs through filter
output


e.g 
suppose this is my log
123
1234
123456

message> 123
message> 1234
message> 123456

Grok parse >> target message {GROK PATTER}

GROK LEARNING DOC:
https://github.com/mussawirimam/Elastic_SearchChintaman/blob/main/grok%20learning.docx

### for now we will stop the logstash and change the directory 

â€ ^C[WARN ] 2025-08-03 17:19:36.443 [SIGINT handler] runner - SIGINT received. Shutting down.
[WARN ] 2025-08-03 17:19:41.447 [Ruby-0-Thread-19: /usr/share/logstash/logstash-core/lib/logstash/runner.rb:624] runner - Received shutdown signal, but pipeline is still waiting for in-flight events
to be processed. Sending another ^C will force quit Logstash, but this may cause
data loss.
[INFO ] 2025-08-03 17:19:43.205 [[main]-pipeline-manager] javapipeline - Pipeline terminated {"pipeline.id"=>"main"}
[INFO ] 2025-08-03 17:19:43.504 [Converge PipelineAction::StopAndDelete<main>] pipelinesregistry - Removed pipeline from registry successfully {:pipeline_id=>:main}
[INFO ] 2025-08-03 17:19:43.526 [LogStash::Runner] runner - Logstash shut down.
root@elk:/etc/logstash# cd /etc/logstash/
root@elk:/etc/logstash# vim /etc/logstash/conf.d/
root@elk:/etc/logstash# vim /etc/logstash/conf.d/filebeat.conf

### currently our filebeat.conf is setup like this, we arent doing any filteration on it.
### now what we have to understand is how to specify a filter
root@elk:/etc/logstash# cat /etc/logstash/conf.d/filebeat.conf
input {
  beats {    <----------- filebeat will be an input. Which will send the data to logstash. And, through logstash data will be parsed and sent to elasticsearch as output
    port => 5044  <-------------------- logstash port

  }
}

filter {
    <-------------------------- ### we are going to work here now, we will learn how to specify the filter in logstash
}

output
{

stdout {
  codec => rubydebug
}

  elasticsearch {
    hosts => ["https://sys.fcc:9200"]  <------------------- host to which the output will be sent
    index => "logs_for_test_%{+YYYY.MM.dd}"  <---------------------- date pattern
    user => elastic    <----------------------------- username
    password => blfOwgJUMUuLKjmXVYU2  <------------------------------- password
    ssl => true  <------------------------------------------- ssl enabled
    cacert => "/etc/elasticsearch/certs/ca/ca.crt"  <--------------------------- location of the ca certificate
  }
}

### let's suppose there are unwatned field that I am able to see, let's suppose I have decided which fields I dont want. We will use the filter for it. 
### we have this document as an e.g that is coming through the filebeat, we will idenify that we dont want some of the fields coming in through the logs that the filebeat is sending over to logstash and then logstash parsing that data to elasticsearch.
### we will use the filter to drop some of these fields.
# SAMPLE JSON event document from kibana UI:
{
  "_index": "logs_for_test_2025.08.03",
  "_id": "voXncJgBh6oPYmnzTH_S",
  "_version": 1,
  "_source": {
    "agent": {
      "id": "d7a4dfcb-de4e-4bbf-b0a1-060c65dab621",
      "ephemeral_id": "deba83c2-edba-4aad-be61-fcaa8810305e",
      "name": "elk",
      "type": "filebeat",
      "version": "8.18.1"
    },
    "ecs": {
      "version": "8.0.0"
    },
    "log": {
      "offset": 20,
      "file": {
        "device_id": "64512",
        "inode": "131799",
        "path": "/var/log/hello.log"
      }
    },
    "@timestamp": "2025-08-03T17:07:33.091Z",
    "@version": "1",
    "tags": [
      "beats_input_codec_plain_applied"
    ],
    "input": {
      "type": "filestream"
    },
    "message": "my next line",
    "event": {
      "original": "my next line"
    },
    "host": {
      "id": "53d351d845dc4ea09111e51cc53b4b62",
      "ip": [
        "10.211.55.32",
        "fdb2:2c26:f4e4:0:21c:42ff:fe6d:e089",
        "fe80::21c:42ff:fe6d:e089"
      ],
      "containerized": false,
      "hostname": "elk",
      "name": "elk",
      "mac": [
        "00-1C-42-6D-E0-89"
      ],
      "os": {
        "platform": "ubuntu",
        "family": "debian",
        "type": "linux",
        "codename": "noble",
        "name": "Ubuntu",
        "kernel": "6.8.0-64-generic",
        "version": "24.04.2 LTS (Noble Numbat)"
      },
      "architecture": "aarch64"
    }
  },
  "fields": {
    "agent.version.keyword": [
      "8.18.1"
    ],
    "host.architecture.keyword": [
      "aarch64"
    ],
    "host.name.keyword": [
      "elk"
    ],
    "host.hostname": [
      "elk"
    ],
    "host.mac": [
      "00-1C-42-6D-E0-89"
    ],
    "ecs.version.keyword": [
      "8.0.0"
    ],
    "host.ip.keyword": [
      "10.211.55.32",
      "fdb2:2c26:f4e4:0:21c:42ff:fe6d:e089",
      "fe80::21c:42ff:fe6d:e089"
    ],
    "host.os.version": [
      "24.04.2 LTS (Noble Numbat)"
    ],
    "host.os.name": [
      "Ubuntu"
    ],
    "agent.name": [
      "elk"
    ],
    "host.id.keyword": [
      "53d351d845dc4ea09111e51cc53b4b62"
    ],
    "host.name": [
      "elk"
    ],
    "host.os.version.keyword": [
      "24.04.2 LTS (Noble Numbat)"
    ],
    "event.original": [
      "my next line"
    ],
    "host.os.type": [
      "linux"
    ],
    "agent.id.keyword": [
      "d7a4dfcb-de4e-4bbf-b0a1-060c65dab621"
    ],
    "@version.keyword": [
      "1"
    ],
    "input.type": [
      "filestream"
    ],
    "log.offset": [
      20
    ],
    "tags": [
      "beats_input_codec_plain_applied"
    ],
    "host.architecture": [
      "aarch64"
    ],
    "agent.id": [
      "d7a4dfcb-de4e-4bbf-b0a1-060c65dab621"
    ],
    "ecs.version": [
      "8.0.0"
    ],
    "host.containerized": [
      false
    ],
    "message.keyword": [
      "my next line"
    ],
    "host.hostname.keyword": [
      "elk"
    ],
    "agent.version": [
      "8.18.1"
    ],
    "host.os.family": [
      "debian"
    ],
    "input.type.keyword": [
      "filestream"
    ],
    "tags.keyword": [
      "beats_input_codec_plain_applied"
    ],
    "log.file.inode.keyword": [
      "131799"
    ],
    "host.ip": [
      "10.211.55.32",
      "fdb2:2c26:f4e4:0:21c:42ff:fe6d:e089",
      "fe80::21c:42ff:fe6d:e089"
    ],
    "agent.type": [
      "filebeat"
    ],
    "host.os.kernel.keyword": [
      "6.8.0-64-generic"
    ],
    "host.os.kernel": [
      "6.8.0-64-generic"
    ],
    "log.file.device_id": [
      "64512"
    ],
    "@version": [
      "1"
    ],
    "host.os.name.keyword": [
      "Ubuntu"
    ],
    "log.file.device_id.keyword": [
      "64512"
    ],
    "host.id": [
      "53d351d845dc4ea09111e51cc53b4b62"
    ],
    "log.file.path.keyword": [
      "/var/log/hello.log"
    ],
    "agent.type.keyword": [
      "filebeat"
    ],
    "agent.ephemeral_id.keyword": [
      "deba83c2-edba-4aad-be61-fcaa8810305e"
    ],
    "host.os.codename.keyword": [
      "noble"
    ],
    "host.mac.keyword": [
      "00-1C-42-6D-E0-89"
    ],
    "agent.name.keyword": [
      "elk"
    ],
    "host.os.codename": [
      "noble"
    ],
    "message": [
      "my next line"
    ],
    "host.os.family.keyword": [
      "debian"
    ],
    "@timestamp": [
      "2025-08-03T17:07:33.091Z"
    ],
    "host.os.type.keyword": [
      "linux"
    ],
    "host.os.platform": [
      "ubuntu"
    ],
    "host.os.platform.keyword": [
      "ubuntu"
    ],
    "log.file.inode": [
      "131799"
    ],
    "event.original.keyword": [
      "my next line"
    ],
    "log.file.path": [
      "/var/log/hello.log"
    ],
    "agent.ephemeral_id": [
      "deba83c2-edba-4aad-be61-fcaa8810305e"
    ]
  }
}

### google: remove field logstash filter
https://discuss.elastic.co/t/how-to-remove-fields-in-logstash-es/77039/2
#add this to your conf.d/filebeat.conf file.


root@elk:/etc/logstash/conf.d# ls
filebeat.conf
root@elk:/etc/logstash/conf.d# vim filebeat.conf


root@elk:/etc/logstash/conf.d# cat filebeat.conf
input {
  beats {
    port => 5044

  }
}

filter {
  mutate { remove_field => [ "field1", "field2", "field3", ... "fieldN" ] }  <-------------------------------- ### we added this line, now what we have to do is to add the fields that we want to mutate from our json data from the log documents
}

output
{

stdout {
  codec => rubydebug
}

  elasticsearch {
    hosts => ["https://sys.fcc:9200"]
    index => "logs_for_test_%{+YYYY.MM.dd}"
    user => elastic
    password => blfOwgJUMUuLKjmXVYU2
    ssl => true
    cacert => "/etc/elasticsearch/certs/ca/ca.crt"
  }
}

---------------------------------------------------------------------
FILTERS:
---------------------------------------------------------------------

### mutate is one of the filter plugin
### in mutatation what do I have to do? In mutation many things can happen.
### google search: all mutate logstash filters (lets use offical documentation)
https://www.elastic.co/docs/reference/logstash/plugins/plugins-filters-mutate

### this is the directory where all the file gets read by the filebeat and send to the output which logstash and logstash can process it to the elk
### currently we did not do any filteration on the conf file, the data is very unstructured in logstash. 
### we are running the logstash as the BINARY: /usr/share/logstash/bin/logstash -f /etc/logstash/conf.d/filebeat.conf (later we will use it as a DAEMON)
### so we will see how the logs are going to inject in elasticsearch and then once we apply filter how the data is going to look like. 

filebeat.conf
....
output
{

stdout {
  codec => rubydebug
}

  elasticsearch {
    hosts => ["https://sys.fcc:9200"]
    index => "logs_for_test_%{+YYYY.MM.dd}" <------------- # in our pipeeline we gave the index name logs_for_test and the and on Kibana UI, we created dataview on particular index and then we used date as the suffix. And we added a index pattern at the time of creating dataview on kibana. So any log that gets sent from the filebeat to the elasticsearch gets visualized in the dataview.
    user => elastic
    password => blfOwgJUMUuLKjmXVYU2
    ssl => true
    cacert => "/etc/elasticsearch/certs/ca/ca.crt"
  }
}

### Sample data and filters:
https://github.com/mussawirimam/Elastic_SearchChintaman/blob/main/Cooman%20Logstash%20fillters.docx

### we are going to create a sample log file in filebeat server in the defined directory location that's been configured and we will apply the filter in the logstash server:
root@elk:/var/log# vim test1.log
127.0.0.1 - - frank [10/Oct/2024:13:55:36 -0700] "GET /apache_pb.gif HTTP/1.0" 200 
:wq!


### when using a Grok what happens is that behind the scene for grok there is a regex.
### regex is used for finding the pattern. 
### most of the regex are available in the GROK patterns
https://github.com/logstash-plugins/logstash-patterns-core

----------------------------
TOOLS FOR REGEX AND GROK:
https://regexr.com
https://grokdebugger.com
----------------------------
GROK Pattern:
%{TIMESTAMP_ISO8601:log_timestamp} %{LOGLEVEL:log_level} \[%{DATA:service}\] - %{GREEDYDATA:message}

Sample raw Data:
025-03-10 14:25:43,123 INFO [order-service] - Order ID 4567 processed successfully in 120ms
2025-03-10 14:25:44,789 ERROR [payment-service] - Payment ID 8910 failed due to insufficient funds

JSON OUTPUT after applying GROK pattern:
[
  {
    "log_timestamp": "25-03-10 14:25:43,123",
    "log_level": "INFO",
    "service": "order-service",
    "message": "Order ID 4567 processed successfully in 120ms"
  },
  {
    "log_timestamp": "2025-03-10 14:25:44,789",
    "log_level": "ERROR",
    "service": "payment-service",
    "message": "Payment ID 8910 failed due to insufficient funds"
  }
]

--------------------
DECONSTRUCTING GROK Pattern:
--------------------
GROK Pattern:
%{TIMESTAMP_ISO8601:log_timestamp} %{LOGLEVEL:log_level} \[%{DATA:service}\] - %{GREEDYDATA:message}

Sample raw Data:
%{TIMESTAMP_ISO8601:log_timestamp} =  025-03-10 14:25:43,123 
%{LOGLEVEL:log_level} = INFO 
\[%{DATA:service}\] = [order-service] 
- %{GREEDYDATA:message} = - Order ID 4567 processed successfully in 120ms


%{TIMESTAMP_ISO8601:log_timestamp} = 2025-03-10 14:25:44,789
%{LOGLEVEL:log_level} =  ERROR 
\[%{DATA:service}\] = [payment-service]
- %{GREEDYDATA:message} = - Payment ID 8910 failed due to insufficient funds

JSON OUTPUT after applying GROK pattern:
[
  {
    "log_timestamp": "25-03-10 14:25:43,123",  <----------------------- all the fieldnames are coming from the name we have defined for the GROK if you notice above. %{TIMESTAMP_ISO8601 <---------- collects the raw data information  :log_timestamp <-------------- gives the fieldname} 
    "log_level": "INFO",
    "service": "order-service",
    "message": "Order ID 4567 processed successfully in 120ms"
  },
  {
    "log_timestamp": "2025-03-10 14:25:44,789",
    "log_level": "ERROR",
    "service": "payment-service",
    "message": "Payment ID 8910 failed due to insufficient funds"
  }
]

---------------------------------
VERY IMPORTANT:
### look how it is catching the phrases through the GrokPattern (under the hood regex is the player)
---------------------------------
UNSTRUCTURED DATA TO STRUCTURED DATA:
---------------------------------
UNSTRUCTURED:
127.0.0.1 - frank [10/Oct/2000:13:55:36 -0700] "GET /apache_pb.gif HTTP/1.0" 200 2326

GROK PATTERN
%{IP:IP} - %{DATA:Name} \[%{HTTPDATE:timestamp}\] "%{DATA:Request} %{DATA:Uri}HTTP/1.0" %{NUMBER:Responce} %{NUMBER:Pid}

STRUCTURED:
[
  {
    "IP": "127.0.0.1",
    "Name": "frank",
    "timestamp": "10/Oct/2000:13:55:36 -0700",
    "Request": "GET",
    "Uri": "/apache_pb.gif ",
    "Responce": 200,
    "Pid": 2326
  }
]

### NOW WE ARE GOING TO IMPLEMET THIS AS PARSER IN THE FILEBEAT.CONF FILE ON LOGSTASH SERVER
### WE WILL PROVIDE A FILTER

root@elk:/etc/logstash/conf.d# cat filebeat.conf
input {
  beats {
    port => 5044

  }
}

filter {

grok {

match => {"message" => "%{IP:IP} - %{DATA:Name} \[%{HTTPDATE:timestamp}\] "%{DATA:Request} %{DATA:Uri}HTTP/1.0" %{NUMBER:Responce} %{NUMBER:Pid}" }

}

}

output
{

stdout {
  codec => rubydebug
}

  elasticsearch {
    hosts => ["https://sys.fcc:9200"]
    index => "logs_for_test_%{+YYYY.MM.dd}"
    user => elastic
    password => blfOwgJUMUuLKjmXVYU2
    ssl => true
    cacert => "/etc/elasticsearch/certs/ca/ca.crt"
  }
}

root@elk:/etc/logstash/conf.d#

### run the logstash now 
root@elk:/etc/logstash/conf.d# /usr/share/logstash/bin/logstash -f /etc/logstash/conf.d/filebeat.conf
Using bundled JDK: /usr/share/logstash/jdk
input {
  beats {
    port => 5044

  }
}

filter {

grok {

match => {"message" => "%{IP:IP} - %{DATA:Name} \[%{HTTPDATE:timestamp}\] "%{DATA:Request} %{DATA:Uri}HTTP/1.0" %{NUMBER:Responce} %{NUMBER:Pid}" }

}

}

output
{

stdout {
  codec => rubydebug
}

  elasticsearch {
    hosts => ["https://sys.fcc:9200"]
    index => "logs_for_test_%{+YYYY.MM.dd}"
    user => elastic
"filebeat.conf" 34L, 491B                                                                                                                                                            12,24         Top
WARNING: Could not find logstash.yml which is typically located in $LS_HOME/config or /etc/logstash. You can specify the path using --path.settings. Continuing using the defaults
Could not find log4j2 configuration at path /usr/share/logstash/config/log4j2.properties. Using default config which logs errors to the console
[WARN ] 2025-08-20 23:56:38.985 [main] runner - Starting from version 9.0, running with superuser privileges is not permitted unless you explicitly set 'allow_superuser' to true, thereby acknowledging the possible security risks
[WARN ] 2025-08-20 23:56:38.991 [main] runner - NOTICE: Running Logstash as a superuser is strongly discouraged as it poses a security risk. Set 'allow_superuser' to false for better security.
[WARN ] 2025-08-20 23:56:38.998 [main] runner - 'pipeline.buffer.type' setting is not explicitly defined.Before moving to 9.x set it to 'heap' and tune heap size upward, or set it to 'direct' to maintain existing behavior.
[INFO ] 2025-08-20 23:56:38.999 [main] runner - Starting Logstash {"logstash.version"=>"8.18.4", "jruby.version"=>"jruby 9.4.9.0 (3.1.4) 2024-11-04 547c6b150e OpenJDK 64-Bit Server VM 21.0.7+6-LTS on 21.0.7+6-LTS +indy +jit [aarch64-linux]"}
[INFO ] 2025-08-20 23:56:39.002 [main] runner - JVM bootstrap flags: [-Xms1g, -Xmx1g, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djruby.compile.invokedynamic=true, -XX:+HeapDumpOnOutOfMemoryError, -Djava.security.egd=file:/dev/urandom, -Dlog4j2.isThreadContextMapInheritable=true, -Djruby.regexp.interruptible=true, -Djdk.io.File.enableADS=true, --add-exports=jdk.compiler/com.sun.tools.javac.api=ALL-UNNAMED, --add-exports=jdk.compiler/com.sun.tools.javac.file=ALL-UNNAMED, --add-exports=jdk.compiler/com.sun.tools.javac.parser=ALL-UNNAMED, --add-exports=jdk.compiler/com.sun.tools.javac.tree=ALL-UNNAMED, --add-exports=jdk.compiler/com.sun.tools.javac.util=ALL-UNNAMED, --add-opens=java.base/java.security=ALL-UNNAMED, --add-opens=java.base/java.io=ALL-UNNAMED, --add-opens=java.base/java.nio.channels=ALL-UNNAMED, --add-opens=java.base/sun.nio.ch=ALL-UNNAMED, --add-opens=java.management/sun.management=ALL-UNNAMED, -Dio.netty.allocator.maxOrder=11]
[INFO ] 2025-08-20 23:56:39.135 [main] StreamReadConstraintsUtil - Jackson default value override `logstash.jackson.stream-read-constraints.max-string-length` configured to `200000000` (logstash default)
[INFO ] 2025-08-20 23:56:39.136 [main] StreamReadConstraintsUtil - Jackson default value override `logstash.jackson.stream-read-constraints.max-number-length` configured to `10000` (logstash default)
[INFO ] 2025-08-20 23:56:39.136 [main] StreamReadConstraintsUtil - Jackson default value override `logstash.jackson.stream-read-constraints.max-nesting-depth` configured to `1000` (logstash default)
[WARN ] 2025-08-20 23:56:39.393 [LogStash::Runner] multilocal - Ignoring the 'pipelines.yml' file because modules or command line options are specified
[ERROR] 2025-08-20 23:56:39.821 [Converge PipelineAction::Create<main>] agent - Failed to execute action {:action=>LogStash::PipelineAction::Create/pipeline_id:main, :exception=>"LogStash::ConfigurationError", :message=>"Expected one of [ \\t\\r\\n], \"#\", \"{\", \"-\", [0-9], [A-Za-z_], '\"', \"'\", \"}\" at line 12, column 76 (byte 137) after filter {\n\ngrok {\n\nmatch => {\"message\" => \"%{IP:IP} - %{DATA:Name} \\[%{HTTPDATE:timestamp}\\] \"", :backtrace=>["/usr/share/logstash/logstash-core/lib/logstash/compiler.rb:32:in `compile_imperative'", "org/logstash/execution/AbstractPipelineExt.java:285:in `initialize'", "org/logstash/execution/AbstractPipelineExt.java:223:in `initialize'", "/usr/share/logstash/logstash-core/lib/logstash/java_pipeline.rb:47:in `initialize'", "org/jruby/RubyClass.java:949:in `new'", "/usr/share/logstash/logstash-core/lib/logstash/pipeline_action/create.rb:50:in `execute'", "/usr/share/logstash/logstash-core/lib/logstash/agent.rb:431:in `block in converge_state'"]}
[INFO ] 2025-08-20 23:56:39.860 [Api Webserver] agent - Successfully started Logstash API endpoint {:port=>9601, :ssl_enabled=>false}
[INFO ] 2025-08-20 23:56:39.872 [LogStash::Runner] runner - Logstash shut down.

### we got an error about the match filter, seems like we will have to use the mutate filter instead.

### TROUBLESHOOTING TIME

root@elk:/etc/logstash/conf.d# cat filebeat.conf
input {
  beats {
    port => 5044

  }
}

filter {

grok {

match => {"message" => "%{IP:IP} - %{DATA:Name} \[%{HTTPDATE:timestamp}\] \"%{DATA:Request} %{DATA:Uri}HTTP/1.0\" %{NUMBER:Responce} %{NUMBER:Pid}" } ###<------------------------- added back slash (\) before the quotation marks

}

}

output
{

stdout {
  codec => rubydebug
}

  elasticsearch {
    hosts => ["https://sys.fcc:9200"]
    index => "logs_for_test_%{+YYYY.MM.dd}"
    user => elastic
    password => blfOwgJUMUuLKjmXVYU2
    ssl => true
    cacert => "/etc/elasticsearch/certs/ca/ca.crt"
  }
}

root@elk:/etc/logstash/conf.d#

