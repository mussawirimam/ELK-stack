What is the index?
What is the shard?
What is the replica and what is primaryh shard?
What is the replica and what is the primary shard?
What is the dataview?
What is the documents?
what is the default shard size in elk

Index is a collection of shards. 
We can related it to the database scheme, when we create a database, we create a schema (source of data)
so under scheme multiple tables come, in the elk multiple tables doesnt come shards come in form of table. 

When we read from the schema, we need to create a pattern which is a dataview and this dataview matches the pattern with the index name and provide the document to the interface and we will be able to visualize the documents which is data.

Primary is where data is located and searchable and shard replica is a backup of the primary shard.
shard size, we can create upto 40gb 


‚ùì What is a shard?
üëâ Is a shard a smaller piece of an index?
Exactly! A shard is like a slice of the index. If your data is too big, ELK breaks it into shards to spread across servers and make searching faster.

‚ùì What is a primary shard?
üëâ Is the primary shard the original copy of the data?
Yes! The primary shard holds the main data. Every piece of data goes into the primary shard first.

‚ùì What is a replica shard?
üëâ Is the replica shard a backup of the primary shard?
Yes! A replica shard is just a copy of a primary shard. It's used for backup and faster searching in case one node fails.

‚ùì What is a data view in Kibana?
üëâ Is a data view how I look at my index in Kibana?
Yes! A data view (previously called index pattern) is how you connect Kibana to one or more indexes so you can search, filter, and visualize your data.

‚ùì What is a document in ELK?
üëâ Is a document like one row of data?
Exactly! A document is one single entry or record. Like one log line, one user activity, or one error report.

‚ùì What is the default shard size in elk?
üëâ Is there a fixed size for each shard by default?
Not exactly ‚Äî there‚Äôs no hard size limit, but Elasticsearch recommends keeping each shard under 50GB for best performance.

‚úÖ Best practice: Try to keep shard size around 20‚Äì40 GB, depending on your use case.
So in summary:

There's no strict limit, but...
Too small = too many shards = waste of memory
Too big = slow performance or errors
üß† Think of shards like pizza slices ‚Äî not too many tiny slices, not one giant slice!
Let me know if you want to see how to check shard sizes with a command!

------------------------
INDEX TEMPALTE :
------------------------
- Index template is a predefined format, in which data will be molded as per settings defined in the index template
- if we dont define the index template, then by default index's will be using ecs (elastic common schema)

how the index template matches? we define the index pattern in the settings



------------------------
FILEBEAT ARCHITECTURE:
------------------------
We will create the file beat architecture now:

kibana
logstash
filebeat
webapplication (structured and unstructured data) 
  structured: /opt/test/logs/my-structured.logs
  unstructured: /opt/test/logs/my-unstructured.logs


structure ex: {"name": "shivam"}
unstructured ex: name shivam age 30

anything (data) readable by the machine is structured 
anything (data) not readable by the machine is ustructured 


in elasticsearch there are fleet of agents:

there multiple agents
beats:
apm
etl - logstash

for different purpose we have different beats:
Beats
Filebeat. Lightweight shipper for logs and other data, Metricbeat. Lightweight shipper for metric data.
Packetbeat. Lightweight shipper for network data.
Winlogbeat. Lightweight shipper for Windows event logs.
Auditbeat. Lightweight shipper for audit data.
Heartbeat. Lightweight shipper for uptime monitoring.

we can put multiple processors in the filebeat
INPUT 
  -logs
  -/opts/test/logs/*.log  (inode numbers is the way filebeats rememeber from where the file is being read)
PROCESSORS  
  customization
  adding some tags
    service_name = web
    server_env = prod
  dropping some unwanted data
OUTPUT
  LOGSTASH
  ELK

Logs : information about process or error or debug info of any server or application
Metrics: metric is data through which we can measure system resrouce utilization performance validation
  Cpu, memory, resource, network, IO, storage, and etc
windlogbeat: windows to collect windows events data 
Heartbeat: api, server is up or not is there any lattency or not, url monitoring, etc

------------------------------------------------
FILEBEAT INSTALLATION AND CONFIGURATION
------------------------------------------------
OFFICIAL DOCUMENTATION: https://www.elastic.co/docs/reference/beats/filebeat/setup-repositories

File beat Installation Script for Ubuntu
Save this script as install_filebeat.sh and run it.
sh

#!/bin/bash

# Update package list
sudo apt update -y

# Download and install the Elastic GPG Key
wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo tee /usr/share/keyrings/elasticsearch-keyring.asc

# Add the Elastic APT repository
echo "deb [signed-by=/usr/share/keyrings/elasticsearch-keyring.asc] https://artifacts.elastic.co/packages/8.x/apt stable main" | sudo tee /etc/apt/sources.list.d/elastic-8.x.list

# Update package list again
sudo apt update -y

# Install Filebeat version 8.18.1
sudo apt install -y filebeat=8.18.1

# Enable and start Filebeat service
sudo systemctl enable filebeat
sudo systemctl start filebeat

# Check Filebeat status
sudo systemctl status filebeat --no-pager

# Print installation success message
echo "Filebeat 8.18.1 installation completed successfully!"

 
How to Run the Script
1.	Save the script as install_filebeat.sh.
2.	Give execution permissions:
chmod +x install_filebeat.sh
3.	Run the script:
./install_filebeat.sh
This will install Filebeat 8.18.1, enable the service, and check its status. Let me know if you need additional configurations!

------------------------------------------------------------------------
ONCE THE FILEBEAT IS INSTALLED, YOU NEED TO CONFIGURE THE FILEBEAT
------------------------------------------------------------------------
root@elk:/usr/bin# cd ~
root@elk:~# cd /etc/filebeat/
root@elk:/etc/filebeat# ls
fields.yml  filebeat.reference.yml  filebeat.yml  modules.d
root@elk:/etc/filebeat# vi filebeat.yml
root@elk:/etc/filebeat# cat filebeat.yml
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
###################### Filebeat Configuration Example #########################

# This file is an example configuration file highlighting only the most common
# options. The filebeat.reference.yml file from the same directory contains all the
# supported options with more comments. You can use it as a reference.
#
# You can find the full configuration reference here:
# https://www.elastic.co/guide/en/beats/filebeat/index.html

# For more available modules and options, please see the filebeat.reference.yml sample
# configuration file.

# ============================== Filebeat inputs ===============================

filebeat.inputs:

# Each - is an input. Most options can be set at the input level, so
# you can use different inputs for various configurations.
# Below are the input-specific configurations.

# filestream is an input for collecting log messages from files.
- type: filestream

  # Unique ID among all inputs, an ID is required.
  id: my-filestream-id

  # Change to true to enable this input configuration.
  enabled: false

  # Paths that should be crawled and fetched. Glob based paths.
  paths:
    - /var/log/*.log
    #- c:\programdata\elasticsearch\logs\*

  # Exclude lines. A list of regular expressions to match. It drops the lines that are
  # matching any regular expression from the list.
  # Line filtering happens after the parsers pipeline. If you would like to filter lines
  # before parsers, use include_message parser.
  #exclude_lines: ['^DBG']

  # Include lines. A list of regular expressions to match. It exports the lines that are
  # matching any regular expression from the list.
  # Line filtering happens after the parsers pipeline. If you would like to filter lines
  # before parsers, use include_message parser.
  #include_lines: ['^ERR', '^WARN']

  # Exclude files. A list of regular expressions to match. Filebeat drops the files that
  # are matching any regular expression from the list. By default, no files are dropped.
  #prospector.scanner.exclude_files: ['.gz$']

  # Optional additional fields. These fields can be freely picked
  # to add additional information to the crawled log files for filtering
  #fields:
  #  level: debug
  #  review: 1

# journald is an input for collecting logs from Journald
#- type: journald

  # Unique ID among all inputs, if the ID changes, all entries
  # will be re-ingested
  #id: my-journald-id

  # The position to start reading from the journal, valid options are:
  #  - head: Starts reading at the beginning of the journal.
  #  - tail: Starts reading at the end of the journal.
  #    This means that no events will be sent until a new message is written.
  #  - since: Use also the `since` option to determine when to start reading from.
  #seek: head

  # A time offset from the current time to start reading from.
  # To use since, seek option must be set to since.
  #since: -24h

  # Collect events from the service and messages about the service,
  # including coredumps.
  #units:
    #- docker.service

# ============================== Filebeat modules ==============================

filebeat.config.modules:
  # Glob pattern for configuration loading
  path: ${path.config}/modules.d/*.yml

  # Set to true to enable config reloading
  reload.enabled: false

  # Period on which files under path should be checked for changes
  #reload.period: 10s

# ======================= Elasticsearch template setting =======================

setup.template.settings:
  index.number_of_shards: 1
  #index.codec: best_compression
  #_source.enabled: false


# ================================== General ===================================

# The name of the shipper that publishes the network data. It can be used to group
# all the transactions sent by a single shipper in the web interface.
#name:

# The tags of the shipper are included in their field with each
# transaction published.
#tags: ["service-X", "web-tier"]

# Optional fields that you can specify to add additional information to the
# output.
#fields:
#  env: staging

# ================================= Dashboards =================================
# These settings control loading the sample dashboards to the Kibana index. Loading
# the dashboards is disabled by default and can be enabled either by setting the
# options here or by using the `setup` command.
#setup.dashboards.enabled: false

# The URL from where to download the dashboard archive. By default, this URL
# has a value that is computed based on the Beat name and version. For released
# versions, this URL points to the dashboard archive on the artifacts.elastic.co
# website.
#setup.dashboards.url:

# =================================== Kibana ===================================

# Starting with Beats version 6.0.0, the dashboards are loaded via the Kibana API.
# This requires a Kibana endpoint configuration.
setup.kibana:

  # Kibana Host
  # Scheme and port can be left out and will be set to the default (http and 5601)
  # In case you specify and additional path, the scheme is required: http://localhost:5601/path
  # IPv6 addresses should always be defined as: https://[2001:db8::1]:5601
  #host: "localhost:5601"

  # Kibana Space ID
  # ID of the Kibana Space into which the dashboards should be loaded. By default,
  # the Default Space will be used.
  #space.id:

# =============================== Elastic Cloud ================================

# These settings simplify using Filebeat with the Elastic Cloud (https://cloud.elastic.co/).

# The cloud.id setting overwrites the `output.elasticsearch.hosts` and
# `setup.kibana.host` options.
# You can find the `cloud.id` in the Elastic Cloud web UI.
#cloud.id:

# The cloud.auth setting overwrites the `output.elasticsearch.username` and
# `output.elasticsearch.password` settings. The format is `<user>:<pass>`.
#cloud.auth:

# ================================== Outputs ===================================

# Configure what output to use when sending the data collected by the beat.

# ---------------------------- Elasticsearch Output ----------------------------
output.elasticsearch:
  # Array of hosts to connect to.
  hosts: ["localhost:9200"]

  # Performance preset - one of "balanced", "throughput", "scale",
  # "latency", or "custom".
  preset: balanced

  # Protocol - either `http` (default) or `https`.
  #protocol: "https"

  # Authentication credentials - either API key or username/password.
  #api_key: "id:api_key"
  #username: "elastic"
  #password: "changeme"

# ------------------------------ Logstash Output -------------------------------
#output.logstash:
  # The Logstash hosts
  #hosts: ["localhost:5044"]

  # Optional SSL. By default is off.
  # List of root certificates for HTTPS server verifications
  #ssl.certificate_authorities: ["/etc/pki/root/ca.pem"]

  # Certificate for SSL client authentication
  #ssl.certificate: "/etc/pki/client/cert.pem"

  # Client Certificate Key
  #ssl.key: "/etc/pki/client/cert.key"

# ================================= Processors =================================
processors:
  - add_host_metadata:
      when.not.contains.tags: forwarded
  - add_cloud_metadata: ~
  - add_docker_metadata: ~
  - add_kubernetes_metadata: ~

# ================================== Logging ===================================

# Sets log level. The default log level is info.
# Available log levels are: error, warning, info, debug
#logging.level: debug

# At debug level, you can selectively enable logging only for some components.
# To enable all selectors, use ["*"]. Examples of other selectors are "beat",
# "publisher", "service".
#logging.selectors: ["*"]

# ============================= X-Pack Monitoring ==============================
# Filebeat can export internal metrics to a central Elasticsearch monitoring
# cluster.  This requires xpack monitoring to be enabled in Elasticsearch.  The
# reporting is disabled by default.

# Set to true to enable the monitoring reporter.
#monitoring.enabled: false

# Sets the UUID of the Elasticsearch cluster under which monitoring data for this
# Filebeat instance will appear in the Stack Monitoring UI. If output.elasticsearch
# is enabled, the UUID is derived from the Elasticsearch cluster referenced by output.elasticsearch.
#monitoring.cluster_uuid:

# Uncomment to send the metrics to Elasticsearch. Most settings from the
# Elasticsearch outputs are accepted here as well.
# Note that the settings should point to your Elasticsearch *monitoring* cluster.
# Any setting that is not set is automatically inherited from the Elasticsearch
# output configuration, so if you have the Elasticsearch output configured such
# that it is pointing to your Elasticsearch monitoring cluster, you can simply
# uncomment the following line.
#monitoring.elasticsearch:

# ============================== Instrumentation ===============================

# Instrumentation support for the filebeat.
#instrumentation:
    # Set to true to enable instrumentation of filebeat.
    #enabled: false

    # Environment in which filebeat is running on (eg: staging, production, etc.)
    #environment: ""

    # APM Server hosts to report instrumentation results to.
    #hosts:
    #  - http://localhost:8200

    # API Key for the APM Server(s).
    # If api_key is set then secret_token will be ignored.
    #api_key:

    # Secret token for the APM Server(s).
    #secret_token:


# ================================= Migration ==================================

# This allows to enable 6.7 migration aliases
#migration.6_to_7.enabled: true

root@elk:/etc/filebeat#

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

in filebeat there are two configuration that you will be interacting mostly 
/etc/filebeat < mandatory file
filebeat.yml 
  input custom:
    /opt/var/log1
    /opt/var/log2
  modules:
    iis.yml.enabled
    mysql.yml.disabled

### this is the section where you specify the module section. 
root@elk:/etc/filebeat# cat filebeat.yml | grep module
# For more available modules and options, please see the filebeat.reference.yml sample
# ============================== Filebeat modules ==============================
filebeat.config.modules:
  path: ${path.config}/modules.d/*.yml      <-----------------------------------------------------------
root@elk:/etc/filebeat#

### if we dont want to use the module
you can comment out the filebeat.config.modules and the path


### this is where the all the modules exist. You can enable and disable these modules.
root@elk:/etc/filebeat# more filebeat.yml |grep module
# For more available modules and options, please see the filebeat.reference.yml sample
# ============================== Filebeat modules ==============================
filebeat.config.modules:
  path: ${path.config}/modules.d/*.yml
root@elk:/etc/filebeat# cd modules.d/
root@elk:/etc/filebeat/modules.d# ls
activemq.yml.disabled    azure.yml.disabled       crowdstrike.yml.disabled    gcp.yml.disabled               iis.yml.disabled       logstash.yml.disabled   mysql.yml.disabled            o365.yml.disabled     pensando.yml.disabled    santa.yml.disabled     threatintel.yml.disabled
apache.yml.disabled      cef.yml.disabled         cyberarkpas.yml.disabled    google_workspace.yml.disabled  iptables.yml.disabled  microsoft.yml.disabled  mysqlenterprise.yml.disabled  okta.yml.disabled     postgresql.yml.disabled  snyk.yml.disabled      traefik.yml.disabled
auditd.yml.disabled      checkpoint.yml.disabled  elasticsearch.yml.disabled  haproxy.yml.disabled           juniper.yml.disabled   misp.yml.disabled       nats.yml.disabled             oracle.yml.disabled   rabbitmq.yml.disabled    sophos.yml.disabled    zeek.yml.disabled
aws.yml.disabled         cisco.yml.disabled       envoyproxy.yml.disabled     ibmmq.yml.disabled             kafka.yml.disabled     mongodb.yml.disabled    netflow.yml.disabled          osquery.yml.disabled  redis.yml.disabled       suricata.yml.disabled  zookeeper.yml.disabled
awsfargate.yml.disabled  coredns.yml.disabled     fortinet.yml.disabled       icinga.yml.disabled            kibana.yml.disabled    mssql.yml.disabled      nginx.yml.disabled            panw.yml.disabled     salesforce.yml.disabled  system.yml.disabled    zoom.yml.disabled
root@elk:/etc/filebeat/modules.d# ^C
root@elk:/etc/filebeat/modules.d#

### example module
root@elk:/etc/filebeat/modules.d# more mysql.yml.disabled
# Module: mysql
# Docs: https://www.elastic.co/guide/en/beats/filebeat/8.x/filebeat-module-mysql.html

- module: mysql
  # Error logs
  error:
    enabled: false

    # Set custom paths for the log files. If left empty,
    # Filebeat will choose the paths depending on your OS.
    #var.paths:

  # Slow logs
  slowlog:
    enabled: false

    # Set custom paths for the log files. If left empty,
    # Filebeat will choose the paths depending on your OS.
    #var.paths:
root@elk:/etc/filebeat/modules.d#

------------------------------------------------------------------------
LOGSTASH INSTALLATION SCRIPT FOR UBUNTU
------------------------------------------------------------------------

Here are the steps to install Logstash on the latest Ubuntu version:
Step 1: Update System Packages
sh
sudo apt update && sudo apt upgrade -y
Step 2: Install Java (if not installed)
Logstash requires Java. Install OpenJDK:
sh

sudo apt install -y openjdk-17-jdk
Verify Java installation:
sh

java -version
Step 3: Add Elasticsearch Repository
sh

wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo tee /usr/share/keyrings/elasticsearch-keyring.asc
echo "deb [signed-by=/usr/share/keyrings/elasticsearch-keyring.asc] https://artifacts.elastic.co/packages/8.x/apt stable main" | sudo tee /etc/apt/sources.list.d/elastic-8.x.list
Step 4: Install Logstash
sh

sudo apt update
sudo apt install -y logstash
Step 5: Configure Logstash
Edit the main configuration file:
sh

sudo nano /etc/logstash/logstash.yml
Modify settings as needed (e.g., pipeline configurations).
Step 6: Configure Logstash Pipeline
Create a sample pipeline:
sh

### we are configuring the filebeat
sudo nano /etc/logstash/conf.d/sample.conf
Add an example pipeline configuration:
plaintext

input {
  beats {
    port => 5044
  }
}

filter {
  mutate {
    add_field => { "example_field" => "Hello Logstash" }
  }
}

output {
  elasticsearch {
    hosts => ["http://localhost:9200"]
    index => "logstash-test"
  }
  stdout { codec => rubydebug }
}
Save and exit.
Step 7: Enable and Start Logstash
sh

sudo systemctl enable logstash
sudo systemctl start logstash
Check service status:
sh

sudo systemctl status logstash
Step 8: Verify Logstash
Test Logstash manually:
sh

echo '{"message": "Hello, Logstash!"}' | /usr/share/logstash/bin/logstash -f /etc/logstash/conf.d/sample.conf
Step 9: Allow Ports in Firewall (If Required)
sh

sudo ufw allow 5044/tcp
sudo ufw reload
Step 10: Monitor Logstash Logs
sh

sudo journalctl -u logstash -f
Let me know if you need further customization! 





